# ì„œìš¸ì¼€ì–´í”ŒëŸ¬ìŠ¤ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ê³„ ë° ê°œì„  ë°©ì•ˆ

## ğŸ“‹ ëª©ì°¨
1. [í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„](#1-í˜„ì¬-ì‹œìŠ¤í…œ-ë¶„ì„)
2. [ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ì„¤ê³„](#2-ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤-ì•„í‚¤í…ì²˜-ì„¤ê³„)
3. [ë³´ì•ˆ ì·¨ì•½ì  ë° ê°œì„  ë°©ì•ˆ](#3-ë³´ì•ˆ-ì·¨ì•½ì -ë°-ê°œì„ -ë°©ì•ˆ)
4. [ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œì„ ](#4-ë°ì´í„°-íŒŒì´í”„ë¼ì¸-ê°œì„ )
5. [ì„¸ë¶„í™”ê°€ í•„ìš”í•œ ê¸°ëŠ¥](#5-ì„¸ë¶„í™”ê°€-í•„ìš”í•œ-ê¸°ëŠ¥)
6. [êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ë¡œë“œë§µ](#6-êµ¬í˜„-ìš°ì„ ìˆœìœ„-ë°-ë¡œë“œë§µ)

---

## 1. í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### 1.1 ê¸°ì¡´ êµ¬ì¡° í‰ê°€

**í˜„ì¬ êµ¬ì¡° (GitHub ì €ì¥ì†Œ ê¸°ì¤€)**:
```
integrated-care-platform/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ admin/          # ìš´ì˜ ì½˜ì†”
â”‚   â”œâ”€â”€ api/            # BFF/API ì„œë²„
â”‚   â””â”€â”€ web/            # ì›¹ í”„ë¡ íŠ¸ì—”ë“œ
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ data-pipeline/  # ETL íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ geo-engine/     # ì§€ë¦¬ ì—”ì§„
â”‚   â”œâ”€â”€ shared/         # ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬
â”‚   â””â”€â”€ trust-safety/   # ì‹ ë¢°/ì•ˆì „ ëª¨ë“ˆ
â””â”€â”€ infra/
    â”œâ”€â”€ docker/
    â”œâ”€â”€ k8s/
    â””â”€â”€ terraform/
```

**ì¥ì **:
- âœ… ëª¨ë…¸ë ˆí¬(Monorepo) êµ¬ì¡°ë¡œ ì½”ë“œ ê³µìœ  ìš©ì´
- âœ… ë„ë©”ì¸ë³„ íŒ¨í‚¤ì§€ ë¶„ë¦¬ (data-pipeline, geo-engine, trust-safety)
- âœ… Python 3.11+ ì‚¬ìš©ìœ¼ë¡œ ìµœì‹  ê¸°ëŠ¥ í™œìš©
- âœ… FastAPI ê¸°ë°˜ìœ¼ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
- âœ… í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶• (pytest)

**ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­**:
- âŒ **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬ ë¶€ì¡±**: `apps/api`ê°€ ëª¨ë†€ë¦¬ì‹ BFFë¡œ êµ¬ì„±
- âŒ **ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ëˆ„ë½**: ìŠ¤í‚¤ë§ˆ ë° ê´€ê³„ ì •ì˜ í•„ìš”
- âŒ **ë©”ì‹œì§€ í ë¯¸êµ¬í˜„**: Kafka ì˜ì¡´ì„±ë§Œ ìˆê³  ì‹¤ì œ êµ¬í˜„ ì—†ìŒ
- âŒ **ë³´ì•ˆ ë ˆì´ì–´ ë¶€ì¬**: ì¸ì¦/ì¸ê°€, ì•”í˜¸í™”, API Gateway ë¯¸êµ¬í˜„
- âŒ **ëª¨ë‹ˆí„°ë§ ë¶ˆì™„ì „**: Prometheus clientë§Œ ìˆê³  ì‹¤ì œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë¯¸êµ¬í˜„
- âŒ **API ìŠ¤í™ ë¬¸ì„œí™” ë¶€ì¬**: OpenAPI/Swagger ë¬¸ì„œ ì—†ìŒ

---

## 2. ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ì„¤ê³„

### 2.1 ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Client Layer                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Web Client  â”‚  â”‚ Mobile App   â”‚  â”‚ Admin Portal â”‚                  â”‚
â”‚  â”‚  (Next.js)   â”‚  â”‚ (React Nativeâ”‚  â”‚  (React)     â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚              â”‚              â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Gateway Layer                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Kong Gateway / AWS API Gateway                                     â”‚  â”‚
â”‚  â”‚  - Rate Limiting (100 req/min per user)                             â”‚  â”‚
â”‚  â”‚  - Authentication (JWT Validation)                                  â”‚  â”‚
â”‚  â”‚  - Request Routing                                                  â”‚  â”‚
â”‚  â”‚  - Response Caching (Redis 5min TTL)                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Service Mesh Layer (Istio/Envoy)                        â”‚
â”‚  - Service Discovery                                                       â”‚
â”‚  - Circuit Breaking                                                        â”‚
â”‚  - Distributed Tracing                                                     â”‚
â”‚  - mTLS Encryption                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Microservices Layer                                   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  User Service      â”‚  â”‚  Auth Service      â”‚  â”‚ Facility Service  â”‚   â”‚
â”‚  â”‚  (ì‚¬ìš©ì ê´€ë¦¬)       â”‚  â”‚  (ì¸ì¦/ì¸ê°€)         â”‚  â”‚ (ì‹œì„¤ ì •ë³´)        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Search Service    â”‚  â”‚  Review Service    â”‚  â”‚ Verification Svc  â”‚   â”‚
â”‚  â”‚  (í†µí•© ê²€ìƒ‰)         â”‚  â”‚  (ë¦¬ë·° ê´€ë¦¬)         â”‚  â”‚ (OCR ì¸ì¦)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  AI/ML Service     â”‚  â”‚  Matching Service  â”‚  â”‚ Care Report Svc   â”‚   â”‚
â”‚  â”‚  (ì¶”ì²œ ì—”ì§„)         â”‚  â”‚  (ë§¤ì¹­)             â”‚  â”‚ (ì¼€ì–´ ë¦¬í¬íŠ¸)       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Notification Svc  â”‚  â”‚  Payment Service   â”‚  â”‚ Public API Gatewayâ”‚   â”‚
â”‚  â”‚  (ì•Œë¦¼)             â”‚  â”‚  (ê²°ì œ/ì •ì‚°)         â”‚  â”‚ (ê³µê³µ API í†µí•©)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Analytics Service â”‚  â”‚  Admin Service     â”‚  â”‚ Privacy Service   â”‚   â”‚
â”‚  â”‚  (ë¶„ì„)             â”‚  â”‚  (ê´€ë¦¬ì)           â”‚  â”‚ (ì•ˆì‹¬ë²ˆí˜¸)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Message Queue Layer (Kafka)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ user-events  â”‚  â”‚review-events â”‚  â”‚ care-events  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Data Layer                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PostgreSQL        â”‚  â”‚  MongoDB           â”‚  â”‚ Redis             â”‚   â”‚
â”‚  â”‚  (ê´€ê³„í˜• ë°ì´í„°)     â”‚  â”‚  (ë¬¸ì„œí˜• ë°ì´í„°)     â”‚  â”‚ (ìºì‹œ/ì„¸ì…˜)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Elasticsearch     â”‚  â”‚  TimescaleDB       â”‚  â”‚ S3/CloudFlare R2  â”‚   â”‚
â”‚  â”‚  (ê²€ìƒ‰ ì—”ì§„)         â”‚  â”‚  (ì‹œê³„ì—´ ë°ì´í„°)     â”‚  â”‚ (ê°ì²´ ìŠ¤í† ë¦¬ì§€)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  External Integration Layer                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ë³´ê±´ë³µì§€ë¶€ API     â”‚  â”‚  êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨   â”‚  â”‚ ì„œìš¸ì‹œ Open API   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Naver/Kakao OCR   â”‚  â”‚  Toss Payments     â”‚  â”‚ Twilio/Aligo SMS  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ìƒì„¸ ë¶„í•´ (15ê°œ í•µì‹¬ ì„œë¹„ìŠ¤)

#### ì„œë¹„ìŠ¤ 1: User Service (ì‚¬ìš©ì ê´€ë¦¬)

**ì±…ì„**: ì‚¬ìš©ì ê³„ì • ë° í”„ë¡œí•„ ê´€ë¦¬

**ê¸°ìˆ  ìŠ¤íƒ**:
- Language: Python (FastAPI) or Node.js (NestJS)
- Database: PostgreSQL
- Cache: Redis
- Message Queue: Kafka

**API Endpoints**:
```python
# ì‚¬ìš©ì ê´€ë¦¬
POST   /api/v1/users                    # íšŒì›ê°€ì…
GET    /api/v1/users/{user_id}          # ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
PUT    /api/v1/users/{user_id}          # í”„ë¡œí•„ ìˆ˜ì •
DELETE /api/v1/users/{user_id}          # íšŒì› íƒˆí‡´
GET    /api/v1/users/{user_id}/preferences  # ì‚¬ìš©ì ì„ í˜¸ë„

# ì‚¬ìš©ì ê²€ìƒ‰ (ê´€ë¦¬ììš©)
GET    /api/v1/users/search             # ì‚¬ìš©ì ê²€ìƒ‰
```

**Database Schema**:
```sql
-- PostgreSQL
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    phone_encrypted VARCHAR(255) NOT NULL,
    user_type VARCHAR(20) CHECK (user_type IN ('guardian', 'facility_admin', 'care_worker')),
    profile_data JSONB,
    email_verified BOOLEAN DEFAULT FALSE,
    phone_verified BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    deleted_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_users_email ON users(email) WHERE deleted_at IS NULL;
CREATE INDEX idx_users_phone ON users(phone_encrypted) WHERE deleted_at IS NULL;
CREATE INDEX idx_users_type ON users(user_type);

CREATE TABLE user_preferences (
    preference_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,
    care_type VARCHAR(20) CHECK (care_type IN ('child', 'senior', 'community')),
    location_lat DECIMAL(10,8),
    location_lng DECIMAL(11,8),
    search_radius_km INTEGER DEFAULT 5,
    preferred_facilities JSONB,  -- ì„ í˜¸ ì‹œì„¤ íŠ¹ì„±
    notification_settings JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_user_preferences_user ON user_preferences(user_id);
```

**ì´ë²¤íŠ¸ ë°œí–‰**:
```python
# Kafka Events
- user.created
- user.updated
- user.deleted
- user.preferences.changed
```

---

#### ì„œë¹„ìŠ¤ 2: Auth Service (ì¸ì¦/ì¸ê°€)

**ì±…ì„**: ì¸ì¦, ì¸ê°€, ì„¸ì…˜ ê´€ë¦¬, í† í° ë°œê¸‰

**API Endpoints**:
```python
POST   /api/v1/auth/register            # íšŒì›ê°€ì…
POST   /api/v1/auth/login               # ë¡œê·¸ì¸
POST   /api/v1/auth/logout              # ë¡œê·¸ì•„ì›ƒ
POST   /api/v1/auth/refresh             # í† í° ê°±ì‹ 
POST   /api/v1/auth/verify-email        # ì´ë©”ì¼ ì¸ì¦
POST   /api/v1/auth/verify-phone        # íœ´ëŒ€í° ì¸ì¦
POST   /api/v1/auth/reset-password      # ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •
POST   /api/v1/auth/change-password     # ë¹„ë°€ë²ˆí˜¸ ë³€ê²½
GET    /api/v1/auth/me                  # í˜„ì¬ ì‚¬ìš©ì ì •ë³´
```

**ë³´ì•ˆ ìš”êµ¬ì‚¬í•­**:
```python
# JWT í† í° ì„¤ì •
ACCESS_TOKEN_EXPIRE = 15 * 60  # 15ë¶„
REFRESH_TOKEN_EXPIRE = 7 * 24 * 60 * 60  # 7ì¼

# ë¹„ë°€ë²ˆí˜¸ í•´ì‹±
- bcrypt (cost factor 12)
- Argon2id (ê¶Œì¥)

# Rate Limiting
- ë¡œê·¸ì¸ ì‹œë„: 5íšŒ/5ë¶„ (IP ê¸°ì¤€)
- OTP ë°œì†¡: 3íšŒ/30ë¶„ (ì „í™”ë²ˆí˜¸ ê¸°ì¤€)
- ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •: 3íšŒ/1ì‹œê°„

# 2FA (Two-Factor Authentication)
- SMS OTP (Twilio/Aligo)
- Email OTP
- TOTP (Google Authenticator) - ì„ íƒì‚¬í•­
```

**Database Schema**:
```sql
CREATE TABLE auth_tokens (
    token_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,
    access_token VARCHAR(500) NOT NULL,
    refresh_token VARCHAR(500) NOT NULL,
    device_info JSONB,
    ip_address INET,
    user_agent TEXT,
    expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    revoked_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_auth_tokens_user ON auth_tokens(user_id);
CREATE INDEX idx_auth_tokens_refresh ON auth_tokens(refresh_token);
CREATE INDEX idx_auth_tokens_expires ON auth_tokens(expires_at);

CREATE TABLE login_attempts (
    attempt_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID,
    email VARCHAR(255),
    ip_address INET NOT NULL,
    user_agent TEXT,
    success BOOLEAN NOT NULL,
    failure_reason VARCHAR(100),
    attempted_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_login_attempts_ip ON login_attempts(ip_address, attempted_at);
CREATE INDEX idx_login_attempts_email ON login_attempts(email, attempted_at);

CREATE TABLE otp_codes (
    otp_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(user_id),
    phone_encrypted VARCHAR(255),
    email VARCHAR(255),
    code VARCHAR(6) NOT NULL,
    type VARCHAR(20) CHECK (type IN ('sms', 'email')),
    expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
    verified_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_otp_phone ON otp_codes(phone_encrypted, expires_at);
CREATE INDEX idx_otp_email ON otp_codes(email, expires_at);
```

---

#### ì„œë¹„ìŠ¤ 3: Facility Service (ì‹œì„¤ ì •ë³´ ê´€ë¦¬)

**ì±…ì„**: ì‹œì„¤ ì •ë³´ í†µí•© ê´€ë¦¬, ê³µê³µ API ë°ì´í„° ë™ê¸°í™”

**API Endpoints**:
```python
# ì‹œì„¤ ì¡°íšŒ
GET    /api/v1/facilities                      # ì‹œì„¤ ëª©ë¡ (í˜ì´ì§€ë„¤ì´ì…˜)
GET    /api/v1/facilities/{facility_id}        # ì‹œì„¤ ìƒì„¸
GET    /api/v1/facilities/search               # ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + í•„í„°)
GET    /api/v1/facilities/nearby               # ì£¼ë³€ ì‹œì„¤ (ìœ„ì¹˜ ê¸°ë°˜)
GET    /api/v1/facilities/{facility_id}/stats  # ì‹œì„¤ í†µê³„

# ì‹œì„¤ ê´€ë¦¬ (ìš´ì˜ì/ê´€ë¦¬ì)
POST   /api/v1/facilities                      # ì‹œì„¤ ë“±ë¡
PUT    /api/v1/facilities/{facility_id}        # ì‹œì„¤ ì •ë³´ ìˆ˜ì •
DELETE /api/v1/facilities/{facility_id}        # ì‹œì„¤ ì‚­ì œ
POST   /api/v1/facilities/{facility_id}/verify # ì‹œì„¤ ì¸ì¦

# ì‹œì„¤ ì‚¬ì§„/ë¬¸ì„œ
POST   /api/v1/facilities/{facility_id}/photos # ì‚¬ì§„ ì—…ë¡œë“œ
GET    /api/v1/facilities/{facility_id}/photos # ì‚¬ì§„ ëª©ë¡
DELETE /api/v1/facilities/{facility_id}/photos/{photo_id}
```

**Database Schema**:
```sql
CREATE TABLE facilities (
    facility_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    facility_type VARCHAR(50) CHECK (facility_type IN ('childcare', 'nursing_home', 'community_center', 'daycare')),
    name VARCHAR(255) NOT NULL,
    address JSONB NOT NULL,  -- {road, jibun, detail, postal_code, district}
    location GEOGRAPHY(POINT, 4326) NOT NULL,  -- PostGIS
    public_api_id VARCHAR(100),  -- ê³µê³µ API ì—°ë™ ID
    public_api_source VARCHAR(50),  -- 'mohw', 'nhis', 'seoul_open'
    last_synced_at TIMESTAMP WITH TIME ZONE,
    facility_data JSONB,  -- ì‹œì„¤ íŠ¹í™” ë°ì´í„°
    operating_status VARCHAR(20) CHECK (operating_status IN ('active', 'inactive', 'suspended')),
    verification_status VARCHAR(20) CHECK (verification_status IN ('pending', 'verified', 'rejected')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    deleted_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_facilities_type ON facilities(facility_type);
CREATE INDEX idx_facilities_location ON facilities USING GIST(location);
CREATE INDEX idx_facilities_district ON facilities((address->>'district'));
CREATE INDEX idx_facilities_public_api ON facilities(public_api_source, public_api_id);

-- ì•„ë™ ëŒë´„ ì‹œì„¤ íŠ¹í™” í…Œì´ë¸”
CREATE TABLE childcare_facilities (
    facility_id UUID PRIMARY KEY REFERENCES facilities(facility_id) ON DELETE CASCADE,
    capacity INTEGER,
    teacher_child_ratio VARCHAR(10),  -- "1:5"
    cctv_count INTEGER,
    seoul_certified BOOLEAN DEFAULT FALSE,
    special_programs TEXT[],
    meal_cost INTEGER,
    extended_care BOOLEAN,
    age_range_min INTEGER,
    age_range_max INTEGER,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ë…¸ì¸ ìš”ì–‘ ì‹œì„¤ íŠ¹í™” í…Œì´ë¸”
CREATE TABLE nursing_facilities (
    facility_id UUID PRIMARY KEY REFERENCES facilities(facility_id) ON DELETE CASCADE,
    nhis_grade VARCHAR(1) CHECK (nhis_grade IN ('A', 'B', 'C', 'D', 'E')),
    capacity INTEGER,
    current_occupancy INTEGER,
    care_workers_count INTEGER,
    nurses_count INTEGER,
    doctors_count INTEGER,
    specialized_care TEXT[],  -- ["ì¹˜ë§¤ ì „ë¬¸", "ì•” ì „ë¬¸"]
    amenities TEXT[],
    monthly_cost_range JSONB,  -- {min, max}
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE facility_photos (
    photo_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    facility_id UUID REFERENCES facilities(facility_id) ON DELETE CASCADE,
    photo_url VARCHAR(500) NOT NULL,
    thumbnail_url VARCHAR(500),
    photo_type VARCHAR(20) CHECK (photo_type IN ('exterior', 'interior', 'facility', 'meal', 'activity', 'panorama')),
    is_panorama BOOLEAN DEFAULT FALSE,
    display_order INTEGER,
    uploaded_by UUID REFERENCES users(user_id),
    uploaded_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_facility_photos_facility ON facility_photos(facility_id, display_order);
```

**ê³µê³µ API ì—°ë™ ì „ëµ**:
```python
# ë°ì´í„° ì†ŒìŠ¤
API_SOURCES = {
    'childcare': {
        'name': 'ë³´ê±´ë³µì§€ë¶€ ë³´ìœ¡í†µí•©ì •ë³´',
        'endpoint': 'http://api.childcare.go.kr/mediate/rest/',
        'sync_interval': 'daily',  # ë§¤ì¼ ìƒˆë²½ 3ì‹œ
        'rate_limit': '1000/hour'
    },
    'nursing': {
        'name': 'êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨',
        'endpoint': 'https://www.longtermcare.or.kr/npbs/',
        'sync_interval': 'weekly',  # ë§¤ì£¼ ì¼ìš”ì¼
        'rate_limit': '500/hour'
    },
    'seoul_open': {
        'name': 'ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥',
        'endpoint': 'http://openapi.seoul.go.kr:8088/',
        'sync_interval': 'hourly',  # ì‹¤ì‹œê°„ (ìºì‹± 1ì‹œê°„)
        'rate_limit': '1000/day'
    }
}

# ë°ì´í„° ë™ê¸°í™” ì›Œí¬í”Œë¡œìš°
1. API í˜¸ì¶œ â†’ ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘
2. ë°ì´í„° ì •ì œ ë° ë³€í™˜
3. ë³€ê²½ ê°ì§€ (CDC - Change Data Capture)
4. ì¦ë¶„ ì—…ë°ì´íŠ¸
5. ìºì‹œ ë¬´íš¨í™”
6. ì•Œë¦¼ ë°œì†¡ (ê´€ë¦¬ì)
```

---

#### ì„œë¹„ìŠ¤ 4: Search Service (í†µí•© ê²€ìƒ‰)

**ì±…ì„**: Elasticsearch ê¸°ë°˜ í†µí•© ê²€ìƒ‰, í•„í„°ë§, ì •ë ¬

**ê¸°ìˆ  ìŠ¤íƒ**:
- Search Engine: Elasticsearch 8.x
- Cache: Redis
- Language: Python (FastAPI)

**API Endpoints**:
```python
GET    /api/v1/search                    # í†µí•© ê²€ìƒ‰
GET    /api/v1/search/suggest            # ìë™ì™„ì„±
GET    /api/v1/search/filters            # í•„í„° ì˜µì…˜
POST   /api/v1/search/advanced           # ê³ ê¸‰ ê²€ìƒ‰
GET    /api/v1/search/trending           # ì¸ê¸° ê²€ìƒ‰ì–´
```

**Elasticsearch Index Mapping**:
```json
{
  "mappings": {
    "properties": {
      "facility_id": { "type": "keyword" },
      "facility_type": { "type": "keyword" },
      "name": {
        "type": "text",
        "analyzer": "korean",
        "fields": {
          "keyword": { "type": "keyword" },
          "ngram": {
            "type": "text",
            "analyzer": "ngram_analyzer"
          }
        }
      },
      "address": {
        "type": "text",
        "analyzer": "korean",
        "fields": {
          "keyword": { "type": "keyword" }
        }
      },
      "location": { "type": "geo_point" },
      "district": { "type": "keyword" },
      "grade": { "type": "keyword" },
      "rating": { "type": "float" },
      "review_count": { "type": "integer" },
      "capacity": { "type": "integer" },
      "teacher_child_ratio": { "type": "keyword" },
      "seoul_certified": { "type": "boolean" },
      "tags": { "type": "keyword" },
      "certifications": { "type": "keyword" },
      "amenities": { "type": "keyword" },
      "monthly_cost_min": { "type": "integer" },
      "monthly_cost_max": { "type": "integer" },
      "last_synced_at": { "type": "date" },
      "created_at": { "type": "date" }
    }
  },
  "settings": {
    "analysis": {
      "analyzer": {
        "korean": {
          "type": "custom",
          "tokenizer": "nori_tokenizer",
          "filter": ["lowercase", "nori_readingform"]
        },
        "ngram_analyzer": {
          "type": "custom",
          "tokenizer": "ngram_tokenizer",
          "filter": ["lowercase"]
        }
      },
      "tokenizer": {
        "ngram_tokenizer": {
          "type": "ngram",
          "min_gram": 2,
          "max_gram": 3,
          "token_chars": ["letter", "digit"]
        }
      }
    }
  }
}
```

**ê²€ìƒ‰ ì¿¼ë¦¬ ì˜ˆì‹œ**:
```python
# í…ìŠ¤íŠ¸ ê²€ìƒ‰ + ì§€ë¦¬ ê²€ìƒ‰ + í•„í„°ë§
def search_facilities(
    query: str,
    lat: float,
    lng: float,
    radius_km: int = 5,
    facility_type: Optional[str] = None,
    grade: Optional[List[str]] = None,
    certified_only: bool = False,
    sort_by: str = "relevance"
) -> Dict:
    
    es_query = {
        "query": {
            "bool": {
                "must": [
                    {
                        "multi_match": {
                            "query": query,
                            "fields": ["name^3", "address^2", "tags"],
                            "type": "best_fields",
                            "fuzziness": "AUTO"
                        }
                    },
                    {
                        "geo_distance": {
                            "distance": f"{radius_km}km",
                            "location": {
                                "lat": lat,
                                "lon": lng
                            }
                        }
                    }
                ],
                "filter": []
            }
        },
        "sort": [],
        "size": 20
    }
    
    # í•„í„° ì¶”ê°€
    if facility_type:
        es_query["query"]["bool"]["filter"].append(
            {"term": {"facility_type": facility_type}}
        )
    
    if grade:
        es_query["query"]["bool"]["filter"].append(
            {"terms": {"grade": grade}}
        )
    
    if certified_only:
        es_query["query"]["bool"]["filter"].append(
            {"term": {"seoul_certified": True}}
        )
    
    # ì •ë ¬
    if sort_by == "distance":
        es_query["sort"].append({
            "_geo_distance": {
                "location": {"lat": lat, "lon": lng},
                "order": "asc",
                "unit": "km"
            }
        })
    elif sort_by == "rating":
        es_query["sort"].append({"rating": "desc"})
    elif sort_by == "review_count":
        es_query["sort"].append({"review_count": "desc"})
    else:  # relevance (default)
        es_query["sort"].append({"_score": "desc"})
    
    # Elasticsearch ì¿¼ë¦¬ ì‹¤í–‰
    response = es_client.search(index="facilities", body=es_query)
    
    return {
        "total": response["hits"]["total"]["value"],
        "results": [hit["_source"] for hit in response["hits"]["hits"]]
    }
```

---

#### ì„œë¹„ìŠ¤ 5: Review Service (ë¦¬ë·° ê´€ë¦¬)

**ì±…ì„**: ë¦¬ë·° ì‘ì„±, ì¡°íšŒ, ì‹ ê³ , ë¶„ìŸ ì²˜ë¦¬

**API Endpoints**:
```python
# ë¦¬ë·° CRUD
POST   /api/v1/reviews                          # ë¦¬ë·° ì‘ì„±
GET    /api/v1/reviews/{facility_id}            # ì‹œì„¤ ë¦¬ë·° ëª©ë¡
GET    /api/v1/reviews/{review_id}              # ë¦¬ë·° ìƒì„¸
PUT    /api/v1/reviews/{review_id}              # ë¦¬ë·° ìˆ˜ì •
DELETE /api/v1/reviews/{review_id}              # ë¦¬ë·° ì‚­ì œ

# ë¦¬ë·° ìƒí˜¸ì‘ìš©
POST   /api/v1/reviews/{review_id}/helpful      # ë„ì›€ë¨ í‘œì‹œ
POST   /api/v1/reviews/{review_id}/report       # ë¦¬ë·° ì‹ ê³ 
POST   /api/v1/reviews/{review_id}/reply        # ì‹œì„¤ ì¸¡ ë‹µë³€

# ë¦¬ë·° í†µê³„
GET    /api/v1/reviews/{facility_id}/summary    # ë¦¬ë·° ìš”ì•½ í†µê³„
GET    /api/v1/reviews/{facility_id}/rating-distribution
```

**Database Schema**:
```sql
CREATE TABLE reviews (
    review_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    facility_id UUID REFERENCES facilities(facility_id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(user_id) ON DELETE SET NULL,
    rating INTEGER CHECK (rating BETWEEN 1 AND 5) NOT NULL,
    title VARCHAR(255),
    content TEXT NOT NULL,
    visit_date DATE,
    is_verified BOOLEAN DEFAULT FALSE,
    verification_id UUID,
    verification_method VARCHAR(20) CHECK (verification_method IN ('receipt', 'contract', 'location')),
    trust_score INTEGER CHECK (trust_score BETWEEN 0 AND 100),
    status VARCHAR(20) CHECK (status IN ('active', 'reported', 'blocked', 'deleted')) DEFAULT 'active',
    helpful_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    deleted_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_reviews_facility ON reviews(facility_id, status, created_at DESC);
CREATE INDEX idx_reviews_user ON reviews(user_id);
CREATE INDEX idx_reviews_rating ON reviews(rating);
CREATE INDEX idx_reviews_verified ON reviews(is_verified) WHERE is_verified = TRUE;

CREATE TABLE review_photos (
    photo_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID REFERENCES reviews(review_id) ON DELETE CASCADE,
    photo_url VARCHAR(500) NOT NULL,
    thumbnail_url VARCHAR(500),
    is_panorama BOOLEAN DEFAULT FALSE,
    uploaded_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE review_reports (
    report_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID REFERENCES reviews(review_id) ON DELETE CASCADE,
    reporter_id UUID REFERENCES users(user_id),
    reporter_type VARCHAR(20) CHECK (reporter_type IN ('user', 'facility')),
    reason VARCHAR(50) CHECK (reason IN ('defamation', 'false_info', 'inappropriate', 'spam', 'rights_violation')),
    description TEXT,
    evidence_urls TEXT[],
    status VARCHAR(20) CHECK (status IN ('pending', 'reviewing', 'resolved', 'rejected')) DEFAULT 'pending',
    resolution TEXT,
    reported_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    resolved_at TIMESTAMP WITH TIME ZONE,
    resolved_by UUID REFERENCES users(user_id)
);

CREATE INDEX idx_review_reports_review ON review_reports(review_id);
CREATE INDEX idx_review_reports_status ON review_reports(status);

CREATE TABLE facility_replies (
    reply_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID REFERENCES reviews(review_id) ON DELETE CASCADE,
    facility_id UUID REFERENCES facilities(facility_id),
    user_id UUID REFERENCES users(user_id),  -- ì‹œì„¤ ìš´ì˜ì
    content TEXT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_facility_replies_review ON facility_replies(review_id);
```

**ë¦¬ë·° ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°**:
```python
def calculate_trust_score(review: Dict) -> int:
    """ë¦¬ë·° ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0-100)"""
    score = 0
    
    # 1. ì¸ì¦ ì—¬ë¶€ (+40ì )
    if review.get('is_verified'):
        score += 40
        # ì¸ì¦ ë°©ë²•ë³„ ê°€ì¤‘ì¹˜
        method = review.get('verification_method')
        if method == 'receipt':
            score += 5  # ì˜ìˆ˜ì¦ ì¸ì¦ (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ)
        elif method == 'contract':
            score += 3  # ê³„ì•½ì„œ ì¸ì¦
        elif method == 'location':
            score += 2  # ìœ„ì¹˜ ì¸ì¦
    
    # 2. ì‚¬ì§„ ì²¨ë¶€ (+20ì )
    photo_count = len(review.get('photos', []))
    if photo_count > 0:
        score += min(photo_count * 5, 20)
    
    # 3. ìƒì„¸í•œ ë‚´ìš© (+15ì )
    content_length = len(review.get('content', ''))
    if content_length > 500:
        score += 15
    elif content_length > 200:
        score += 10
    elif content_length > 100:
        score += 5
    
    # 4. ë°©ë¬¸ì¼ ê¸°ë¡ (+10ì )
    if review.get('visit_date'):
        score += 10
    
    # 5. ë„ì›€ë¨ í‘œì‹œ (ìµœëŒ€ +10ì )
    helpful_count = review.get('helpful_count', 0)
    score += min(helpful_count // 2, 10)
    
    # 6. ì‚¬ìš©ì ì‹ ë¢°ë„ (ìµœëŒ€ +5ì )
    user_trust = review.get('user_trust_level', 0)
    score += min(user_trust, 5)
    
    return min(score, 100)
```

**ë¦¬ë·° ë¶„ìŸ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤**:
```
1. ë¦¬ë·° ê²Œì‹œ (Review Posted)
   â†“
2. ì‹ ê³  ì ‘ìˆ˜ (Complaint Filed)
   - ì‹œì„¤ ì¸¡ ì‹ ê³ 
   - ì‚¬ìš©ì ì‹ ê³ 
   â†“
3. ì„ì‹œ ë¸”ë¼ì¸ë“œ (30ì¼) (Temporary Block)
   - ë¦¬ë·° ë¹„ê³µê°œ ì²˜ë¦¬
   - ì‘ì„±ìì—ê²Œ ì†Œëª… ê¸°íšŒ ì œê³µ
   â†“
4. ì–‘ì¸¡ ì¦ë¹™ ì œì¶œ (Proof Submitted)
   - ì‘ì„±ì: ì˜ìˆ˜ì¦, ê³„ì•½ì„œ, ì‚¬ì§„ ë“±
   - ì‹œì„¤: ë°˜ë°• ì¦ê±°, ê¸°ë¡ ë“±
   â†“
5. ê´€ë¦¬ì ê²€í†  (Admin Review)
   - Fact-Based íŒë‹¨
   - ì •ë³´í†µì‹ ë§ë²• ì œ70ì¡° ì¤€ìˆ˜
   â†“
6. ê²°ì •
   â”œâ”€ ë³µêµ¬ (ê³µê³µì˜ ì´ìµ) â†’ ë¦¬ë·° ì¬ê²Œì‹œ
   â””â”€ ì˜êµ¬ ì‚­ì œ (ì•…ì„±/í—ˆìœ„) â†’ ë¦¬ë·° ì˜êµ¬ ì‚­ì œ
```

---

#### ì„œë¹„ìŠ¤ 6: Verification Service (OCR ì¸ì¦)

**ì±…ì„**: OCR ê¸°ë°˜ ì˜ìˆ˜ì¦/ê³„ì•½ì„œ ì¸ì¦, ìœ„ì¹˜ ê¸°ë°˜ ì¸ì¦

**ê¸°ìˆ  ìŠ¤íƒ**:
- OCR: Naver Clova OCR / Google Vision API
- Location: Geofencing (PostGIS)
- Storage: S3 / CloudFlare R2

**API Endpoints**:
```python
POST   /api/v1/verification/receipt            # ì˜ìˆ˜ì¦ ì¸ì¦
POST   /api/v1/verification/contract           # ê³„ì•½ì„œ ì¸ì¦
POST   /api/v1/verification/location           # ìœ„ì¹˜ ì¸ì¦
GET    /api/v1/verification/{verification_id}  # ì¸ì¦ ìƒíƒœ ì¡°íšŒ
POST   /api/v1/verification/{verification_id}/retry  # ì¬ì‹œë„
```

**OCR ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°**:
```
1. ì´ë¯¸ì§€ ì—…ë¡œë“œ
   â†“
2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
   - í•´ìƒë„ ì¡°ì • (ìµœì†Œ 1024x768)
   - íšŒì „ ë³´ì •
   - ë…¸ì´ì¦ˆ ì œê±°
   â†“
3. OCR ì²˜ë¦¬
   - Primary: Naver Clova OCR
   - Fallback: Google Vision API
   â†“
4. ë°ì´í„° ì¶”ì¶œ
   - ì‹œì„¤ëª…
   - ë‚ ì§œ
   - ê¸ˆì•¡
   - ì˜ìˆ˜ì¦/ê³„ì•½ì„œ ë²ˆí˜¸
   â†“
5. ë°ì´í„° ê²€ì¦
   - ì‹œì„¤ DBì™€ ë§¤ì¹­ (ì´ë¦„ ìœ ì‚¬ë„ > 80%)
   - ë‚ ì§œ ìœ íš¨ì„± (ê³¼ê±° 2ë…„ ì´ë‚´)
   - ê¸ˆì•¡ ë²”ìœ„ í™•ì¸
   â†“
6. ì¸ì¦ ì™„ë£Œ
   - ë¦¬ë·°ì™€ ì—°ê²°
   - ì‹ ë¢°ë„ ì ìˆ˜ ìƒìŠ¹
```

**Database Schema**:
```sql
CREATE TABLE verifications (
    verification_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,
    facility_id UUID,
    type VARCHAR(20) CHECK (type IN ('receipt', 'contract', 'location')) NOT NULL,
    status VARCHAR(20) CHECK (status IN ('pending', 'processing', 'verified', 'rejected', 'failed')) DEFAULT 'pending',
    image_url VARCHAR(500),
    ocr_result JSONB,
    extracted_data JSONB,  -- {facility_name, date, amount, receipt_number}
    confidence_score DECIMAL(5,4),  -- 0.0000 ~ 1.0000
    rejection_reason TEXT,
    verified_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_verifications_user ON verifications(user_id);
CREATE INDEX idx_verifications_facility ON verifications(facility_id);
CREATE INDEX idx_verifications_status ON verifications(status);

CREATE TABLE location_verifications (
    location_verification_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    verification_id UUID REFERENCES verifications(verification_id) ON DELETE CASCADE,
    user_location GEOGRAPHY(POINT, 4326),
    facility_location GEOGRAPHY(POINT, 4326),
    distance_meters DECIMAL(10,2),
    is_within_geofence BOOLEAN,
    geofence_radius_meters INTEGER DEFAULT 100,
    verified_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**OCR ê²°ê³¼ ì˜ˆì‹œ**:
```json
{
  "verification_id": "550e8400-e29b-41d4-a716-446655440000",
  "type": "receipt",
  "status": "verified",
  "ocr_result": {
    "provider": "naver_clova",
    "raw_text": "í–‰ë³µì–´ë¦°ì´ì§‘\n2025-02-01\nê¸ˆì•¡: 350,000ì›\nì˜ìˆ˜ì¦ë²ˆí˜¸: 2025020100123",
    "confidence": 0.95,
    "processing_time_ms": 1234
  },
  "extracted_data": {
    "facility_name": "í–‰ë³µì–´ë¦°ì´ì§‘",
    "facility_name_match_score": 0.92,
    "date": "2025-02-01",
    "amount": 350000,
    "receipt_number": "2025020100123",
    "payment_method": "ì¹´ë“œ"
  },
  "confidence_score": 0.95,
  "verified_at": "2025-02-16T10:30:00Z"
}
```

---

#### ì„œë¹„ìŠ¤ 7: AI/ML Service (ì¶”ì²œ ì—”ì§„)

**ì±…ì„**: AI ê¸°ë°˜ ì‹œì„¤ ì¶”ì²œ, ì¥ê¸°ìš”ì–‘ ë“±ê¸‰ ì˜ˆì¸¡, ë³¸ì¸ë¶€ë‹´ê¸ˆ ê³„ì‚°

**ê¸°ìˆ  ìŠ¤íƒ**:
- ML Framework: TensorFlow / PyTorch / Scikit-learn
- Model Serving: TensorFlow Serving / TorchServe
- Feature Store: Feast
- Vector Database: Milvus (ìœ ì‚¬ë„ ê²€ìƒ‰)

**API Endpoints**:
```python
# ì¶”ì²œ
POST   /api/v1/recommend/facilities            # ì‹œì„¤ ì¶”ì²œ
POST   /api/v1/recommend/similar               # ìœ ì‚¬ ì‹œì„¤ ì¶”ì²œ
GET    /api/v1/recommend/personalized/{user_id}  # ê°œì¸í™” ì¶”ì²œ

# ì˜ˆì¸¡
POST   /api/v1/predict/ltc-grade               # ì¥ê¸°ìš”ì–‘ ë“±ê¸‰ ì˜ˆì¸¡
POST   /api/v1/predict/cost-estimate           # ë¹„ìš© ì˜ˆì¸¡
POST   /api/v1/predict/demand-forecast         # ìˆ˜ìš” ì˜ˆì¸¡

# ëª¨ë¸ ê´€ë¦¬
GET    /api/v1/models/status                   # ëª¨ë¸ ìƒíƒœ
POST   /api/v1/models/retrain                  # ì¬í•™ìŠµ íŠ¸ë¦¬ê±°
```

**ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜**:

**1. í˜‘ì—… í•„í„°ë§ (Collaborative Filtering)**
```python
# User-based CF: ìœ ì‚¬í•œ ì‚¬ìš©ìê°€ ì„ íƒí•œ ì‹œì„¤
class UserBasedCF:
    def recommend(self, user_id: str, k: int = 10) -> List[str]:
        # 1. ìœ ì‚¬ ì‚¬ìš©ì ì°¾ê¸° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        similar_users = self.find_similar_users(user_id, top_n=50)
        
        # 2. ìœ ì‚¬ ì‚¬ìš©ìë“¤ì´ ì„ íƒí•œ ì‹œì„¤
        candidate_facilities = self.get_facilities_of_similar_users(similar_users)
        
        # 3. ì ìˆ˜ ê³„ì‚°
        scores = self.calculate_scores(candidate_facilities, similar_users)
        
        # 4. ìƒìœ„ kê°œ ë°˜í™˜
        return sorted(scores, key=lambda x: x[1], reverse=True)[:k]

# Item-based CF: ìœ ì‚¬í•œ ì‹œì„¤ íŠ¹ì„±
class ItemBasedCF:
    def recommend(self, facility_ids: List[str], k: int = 10) -> List[str]:
        # 1. ì‹œì„¤ íŠ¹ì„± ë²¡í„°í™”
        facility_vectors = self.vectorize_facilities(facility_ids)
        
        # 2. ìœ ì‚¬ ì‹œì„¤ ì°¾ê¸°
        similar_facilities = self.find_similar_facilities(facility_vectors)
        
        return similar_facilities[:k]
```

**2. ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ (Content-Based)**
```python
class ContentBasedRecommender:
    def __init__(self):
        self.feature_weights = {
            'location': 0.3,
            'grade': 0.2,
            'rating': 0.2,
            'price': 0.15,
            'amenities': 0.1,
            'certifications': 0.05
        }
    
    def create_user_profile(self, user_id: str) -> np.ndarray:
        """ì‚¬ìš©ì ì„ í˜¸ë„ í”„ë¡œíŒŒì¼ ìƒì„±"""
        user_prefs = self.get_user_preferences(user_id)
        user_history = self.get_user_history(user_id)
        
        # TF-IDF + ê°€ì¤‘í‰ê· 
        profile_vector = self.weighted_average_of_features(
            user_prefs, 
            user_history,
            self.feature_weights
        )
        
        return profile_vector
    
    def recommend(self, user_id: str, k: int = 10) -> List[str]:
        # 1. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±
        user_vector = self.create_user_profile(user_id)
        
        # 2. ëª¨ë“  ì‹œì„¤ ë²¡í„°í™”
        facility_vectors = self.vectorize_all_facilities()
        
        # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(user_vector, facility_vectors)
        
        # 4. ìƒìœ„ kê°œ ë°˜í™˜
        top_k_indices = np.argsort(similarities)[-k:][::-1]
        
        return [self.facility_ids[i] for i in top_k_indices]
```

**3. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸**
```python
class HybridRecommender:
    def __init__(self):
        self.cf_model = UserBasedCF()
        self.cb_model = ContentBasedRecommender()
        
        # ê°€ì¤‘ì¹˜ (í˜‘ì—… í•„í„°ë§ 70%, ì½˜í…ì¸  ê¸°ë°˜ 30%)
        self.cf_weight = 0.7
        self.cb_weight = 0.3
    
    def recommend(self, user_id: str, k: int = 10) -> List[Dict]:
        # 1. CF ì¶”ì²œ
        cf_recommendations = self.cf_model.recommend(user_id, k=20)
        
        # 2. CB ì¶”ì²œ
        cb_recommendations = self.cb_model.recommend(user_id, k=20)
        
        # 3. ì ìˆ˜ ê²°í•©
        combined_scores = {}
        for facility_id, score in cf_recommendations:
            combined_scores[facility_id] = score * self.cf_weight
        
        for facility_id, score in cb_recommendations:
            if facility_id in combined_scores:
                combined_scores[facility_id] += score * self.cb_weight
            else:
                combined_scores[facility_id] = score * self.cb_weight
        
        # 4. ì •ë ¬ ë° ë°˜í™˜
        sorted_recommendations = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [
            {
                'facility_id': facility_id,
                'score': score,
                'facility_info': self.get_facility_info(facility_id)
            }
            for facility_id, score in sorted_recommendations[:k]
        ]
```

**4. ì¥ê¸°ìš”ì–‘ ë“±ê¸‰ ì˜ˆì¸¡ ëª¨ë¸**
```python
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier

class LTCGradePredictor:
    def __init__(self):
        # XGBoost ëª¨ë¸ (ì •í™•ë„ ìš°ì„ )
        self.model = xgb.XGBClassifier(
            max_depth=6,
            learning_rate=0.1,
            n_estimators=100,
            objective='multi:softmax',
            num_class=5  # 1ë“±ê¸‰ ~ 5ë“±ê¸‰
        )
        
        self.feature_names = [
            'age',
            'mobility_score',      # ì´ë™ëŠ¥ë ¥ ì ìˆ˜
            'cognitive_score',     # ì¸ì§€ëŠ¥ë ¥ ì ìˆ˜
            'self_care_score',     # ìê¸°ê´€ë¦¬ ì ìˆ˜
            'daily_living_score',  # ì¼ìƒìƒí™œ ì ìˆ˜
            'chronic_diseases',    # ë§Œì„±ì§ˆí™˜ ê°œìˆ˜
            'requires_assistance', # ë„ì›€ í•„ìš” ì •ë„
            'living_alone'         # ë…ê±° ì—¬ë¶€
        ]
    
    def predict(self, survey_data: Dict) -> Dict:
        """ì„¤ë¬¸ ë°ì´í„° ê¸°ë°˜ ë“±ê¸‰ ì˜ˆì¸¡"""
        # 1. íŠ¹ì„± ì¶”ì¶œ
        features = self.extract_features(survey_data)
        
        # 2. ì˜ˆì¸¡
        predicted_grade = self.model.predict([features])[0] + 1  # 1~5ë“±ê¸‰
        
        # 3. í™•ë¥  ê³„ì‚°
        probabilities = self.model.predict_proba([features])[0]
        
        # 4. ì‹ ë¢°ë„
        confidence = max(probabilities)
        
        return {
            'predicted_grade': predicted_grade,
            'confidence': confidence,
            'probabilities': {
                f'grade_{i+1}': prob 
                for i, prob in enumerate(probabilities)
            },
            'recommendations': self.get_recommendations(predicted_grade)
        }
    
    def extract_features(self, survey_data: Dict) -> List[float]:
        """ì„¤ë¬¸ ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        features.append(survey_data['age'])
        features.append(self.calculate_mobility_score(survey_data))
        features.append(self.calculate_cognitive_score(survey_data))
        features.append(self.calculate_self_care_score(survey_data))
        features.append(self.calculate_daily_living_score(survey_data))
        features.append(len(survey_data.get('chronic_diseases', [])))
        features.append(survey_data.get('requires_assistance', 0))
        features.append(1 if survey_data.get('living_alone') else 0)
        
        return features
```

**5. ë³¸ì¸ë¶€ë‹´ê¸ˆ ê³„ì‚°ê¸°**
```python
class CostCalculator:
    def __init__(self):
        # 2025ë…„ ì¥ê¸°ìš”ì–‘ê¸‰ì—¬ ë¹„ìš© (ì›” í•œë„ì•¡)
        self.rate_table = {
            '1ê¸‰': {
                'home': 1613000,
                'facility': 2064000
            },
            '2ê¸‰': {
                'home': 1414000,
                'facility': 1836000
            },
            '3ê¸‰': {
                'home': 1329000,
                'facility': 1548000
            },
            '4ê¸‰': {
                'home': 1261000,
                'facility': 1417000
            },
            '5ê¸‰': {
                'home': 1002000,
                'facility': 1308000
            }
        }
        
        # ë³¸ì¸ë¶€ë‹´ë¥ 
        self.copayment_rates = {
            'home': 0.15,      # ì¬ê°€ 15%
            'facility': 0.20   # ì‹œì„¤ 20%
        }
        
        # ê°ë©´ ëŒ€ìƒ
        self.exemption_rates = {
            'basic_livelihood': 0.0,   # ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì 0%
            'medical_aid': 0.0,        # ì˜ë£Œê¸‰ì—¬ 0%
            'low_income': 0.50         # ì°¨ìƒìœ„ê³„ì¸µ 50% ê°ë©´
        }
    
    def calculate(
        self,
        grade: str,
        service_type: str,  # 'home' or 'facility'
        exemption_type: Optional[str] = None
    ) -> Dict:
        """ë³¸ì¸ë¶€ë‹´ê¸ˆ ê³„ì‚°"""
        
        # 1. ê¸°ë³¸ ê¸‰ì—¬ì•¡
        base_amount = self.rate_table[grade][service_type]
        
        # 2. ë³¸ì¸ë¶€ë‹´ë¥ 
        copayment_rate = self.copayment_rates[service_type]
        
        # 3. ê°ë©´ ì ìš©
        if exemption_type and exemption_type in self.exemption_rates:
            copayment_rate *= self.exemption_rates[exemption_type]
        
        # 4. ë³¸ì¸ë¶€ë‹´ê¸ˆ
        copayment = int(base_amount * copayment_rate)
        
        # 5. ë¹„ê¸‰ì—¬ ë¹„ìš© (ì‹ë¹„ ë“±)
        non_covered = {
            'meal': 150000,      # ì›” ì‹ë¹„
            'personal': 50000,   # ê°œì¸ìš©í’ˆë¹„
            'utilities': 30000   # ê¸°íƒ€
        }
        
        total_non_covered = sum(non_covered.values())
        
        # 6. ì´ ë¹„ìš©
        total_cost = copayment + total_non_covered
        
        return {
            'grade': grade,
            'service_type': service_type,
            'base_amount': base_amount,
            'copayment_rate': copayment_rate,
            'copayment': copayment,
            'non_covered_costs': non_covered,
            'total_non_covered': total_non_covered,
            'total_monthly_cost': total_cost,
            'breakdown': {
                'covered_by_insurance': base_amount - copayment,
                'user_payment': total_cost
            }
        }
```

---

#### ì„œë¹„ìŠ¤ 8: Matching Service (ë§¤ì¹­ ë° ì˜ˆì•½)

**ì±…ì„**: ì…ì†Œ ì‹ ì²­, ëŒ€ê¸° ê´€ë¦¬, ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜

**API Endpoints**:
```python
# ì‹ ì²­ ê´€ë¦¬
POST   /api/v1/matching/apply                  # ì…ì†Œ/ë“±ë¡ ì‹ ì²­
GET    /api/v1/matching/applications/{user_id} # ì‹ ì²­ ë‚´ì—­
PUT    /api/v1/matching/{match_id}/status      # ìƒíƒœ ë³€ê²½
DELETE /api/v1/matching/{match_id}             # ì‹ ì²­ ì·¨ì†Œ

# ëŒ€ê¸° ê´€ë¦¬
GET    /api/v1/matching/waitlist/{facility_id} # ëŒ€ê¸° ëª©ë¡
POST   /api/v1/matching/waitlist/{facility_id}/join  # ëŒ€ê¸°ì—´ ë“±ë¡
GET    /api/v1/matching/waitlist/my-position   # ë‚´ ëŒ€ê¸° ìˆœìœ„

# ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜
POST   /api/v1/matching/optimal-match          # ìµœì  ë§¤ì¹­ ì¶”ì²œ
```

**Database Schema**:
```sql
CREATE TABLE match_applications (
    application_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,
    facility_id UUID REFERENCES facilities(facility_id) ON DELETE CASCADE,
    application_type VARCHAR(20) CHECK (application_type IN ('reservation', 'waitlist', 'inquiry')) NOT NULL,
    status VARCHAR(20) CHECK (status IN ('pending', 'accepted', 'rejected', 'cancelled', 'expired')) DEFAULT 'pending',
    priority_score INTEGER,
    application_data JSONB,  -- {preferred_date, special_needs, urgency, etc.}
    rejection_reason TEXT,
    applied_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    processed_at TIMESTAMP WITH TIME ZONE,
    expires_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_match_applications_user ON match_applications(user_id, status);
CREATE INDEX idx_match_applications_facility ON match_applications(facility_id, status);
CREATE INDEX idx_match_applications_priority ON match_applications(priority_score DESC);

CREATE TABLE waitlist (
    waitlist_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    facility_id UUID REFERENCES facilities(facility_id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,
    application_id UUID REFERENCES match_applications(application_id),
    position INTEGER,
    estimated_date DATE,
    priority_score INTEGER,
    joined_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    notified_at TIMESTAMP WITH TIME ZONE,
    UNIQUE(facility_id, user_id)
);

CREATE INDEX idx_waitlist_facility ON waitlist(facility_id, position);
CREATE INDEX idx_waitlist_user ON waitlist(user_id);
```

**ìš°ì„ ìˆœìœ„ ì ìˆ˜ ì•Œê³ ë¦¬ì¦˜**:
```python
def calculate_priority_score(application: Dict) -> int:
    """
    ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚° (0-100)
    ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ
    """
    score = 0
    
    # 1. ê¸´ê¸‰ë„ (0-30ì )
    urgency = application.get('urgency')
    if urgency == 'critical':     # ê¸´ê¸‰
        score += 30
    elif urgency == 'high':       # ë†’ìŒ
        score += 20
    elif urgency == 'medium':     # ë³´í†µ
        score += 10
    elif urgency == 'low':        # ë‚®ìŒ
        score += 5
    
    # 2. ê±°ë¦¬ (0-25ì )
    # ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    distance_km = application.get('distance_km', 10)
    distance_score = max(25 - distance_km * 2, 0)
    score += int(distance_score)
    
    # 3. ëŒ€ê¸° ê¸°ê°„ (0-20ì )
    # ì˜¤ë˜ ê¸°ë‹¤ë¦´ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    days_waiting = application.get('days_waiting', 0)
    waiting_score = min(days_waiting // 3, 20)  # 3ì¼ë‹¹ 1ì , ìµœëŒ€ 20ì 
    score += waiting_score
    
    # 4. ì·¨ì•½ê³„ì¸µ ê°€ì  (0-15ì )
    vulnerable_groups = application.get('vulnerable_groups', [])
    if 'basic_livelihood' in vulnerable_groups:  # ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì
        score += 15
    elif 'disabled' in vulnerable_groups:        # ì¥ì• ì¸
        score += 10
    elif 'single_parent' in vulnerable_groups:   # í•œë¶€ëª¨ê°€ì •
        score += 8
    elif 'multicultural' in vulnerable_groups:   # ë‹¤ë¬¸í™”ê°€ì •
        score += 5
    
    # 5. íŠ¹ìˆ˜ ìš”êµ¬ì‚¬í•­ ë§¤ì¹­ (0-10ì )
    special_needs = application.get('special_needs', [])
    facility_amenities = application.get('facility_amenities', [])
    
    # ìš”êµ¬ì‚¬í•­ê³¼ ì‹œì„¤ í¸ì˜ì‹œì„¤ ë§¤ì¹­ë¥ 
    if special_needs and facility_amenities:
        match_rate = len(set(special_needs) & set(facility_amenities)) / len(special_needs)
        score += int(match_rate * 10)
    
    return min(score, 100)
```

**ìµœì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜** (í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ ë³€í˜•):
```python
from scipy.optimize import linear_sum_assignment

class OptimalMatcher:
    def find_optimal_matches(
        self,
        applicants: List[Dict],
        facilities: List[Dict]
    ) -> List[Tuple[str, str, float]]:
        """
        ìµœì  ë§¤ì¹­ ì°¾ê¸°
        - ì‹ ì²­ìì™€ ì‹œì„¤ ê°„ ìµœì  ì¡°í•©
        - ìµœëŒ€ ë§Œì¡±ë„ ë‹¬ì„±
        """
        
        # 1. ë¹„ìš© í–‰ë ¬ ìƒì„± (cost matrix)
        # ë¹„ìš©ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë§¤ì¹­
        n_applicants = len(applicants)
        n_facilities = len(facilities)
        cost_matrix = np.zeros((n_applicants, n_facilities))
        
        for i, applicant in enumerate(applicants):
            for j, facility in enumerate(facilities):
                # ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                match_score = self.calculate_match_score(applicant, facility)
                
                # ë¹„ìš©ìœ¼ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ê²Œ)
                cost_matrix[i][j] = 100 - match_score
        
        # 2. í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ ì ìš©
        row_ind, col_ind = linear_sum_assignment(cost_matrix)
        
        # 3. ê²°ê³¼ ë°˜í™˜
        matches = []
        for i, j in zip(row_ind, col_ind):
            match_score = 100 - cost_matrix[i][j]
            if match_score >= 50:  # ìµœì†Œ ë§¤ì¹­ ì ìˆ˜ 50ì  ì´ìƒ
                matches.append((
                    applicants[i]['user_id'],
                    facilities[j]['facility_id'],
                    match_score
                ))
        
        return matches
    
    def calculate_match_score(
        self,
        applicant: Dict,
        facility: Dict
    ) -> float:
        """
        ì‹ ì²­ì-ì‹œì„¤ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (0-100)
        """
        score = 0
        
        # 1. ê±°ë¦¬ ì ìˆ˜ (30ì )
        distance = self.calculate_distance(
            applicant['location'],
            facility['location']
        )
        distance_score = max(30 - distance * 3, 0)
        score += distance_score
        
        # 2. ì‹œì„¤ ë“±ê¸‰ ì ìˆ˜ (20ì )
        grade_preference = applicant.get('preferred_grade', [])
        if facility['grade'] in grade_preference:
            score += 20
        
        # 3. ì‹œì„¤ íŠ¹ì„± ë§¤ì¹­ (20ì )
        special_needs = set(applicant.get('special_needs', []))
        facility_amenities = set(facility.get('amenities', []))
        
        if special_needs:
            match_rate = len(special_needs & facility_amenities) / len(special_needs)
            score += match_rate * 20
        else:
            score += 20  # íŠ¹ë³„ ìš”êµ¬ì‚¬í•­ ì—†ìœ¼ë©´ ë§Œì 
        
        # 4. ê°€ê²© ë²”ìœ„ ë§¤ì¹­ (15ì )
        budget = applicant.get('budget', {})
        facility_cost = facility.get('monthly_cost', 0)
        
        if budget.get('min', 0) <= facility_cost <= budget.get('max', float('inf')):
            score += 15
        
        # 5. ì‹œì„¤ í‰ì  (15ì )
        rating = facility.get('rating', 0)
        score += (rating / 5.0) * 15
        
        return min(score, 100)
```

---

#### ì„œë¹„ìŠ¤ 9: Care Report Service (ì¼€ì–´ ë¦¬í¬íŠ¸)

**ì±…ì„**: ì‹¤ì‹œê°„ ì¼€ì–´ ê¸°ë¡, ë©€í‹°ëª¨ë‹¬ ë¶„ì„, ì´ìƒ íŒ¨í„´ ê°ì§€

**ê¸°ìˆ  ìŠ¤íƒ**:
- Database: TimescaleDB (ì‹œê³„ì—´)
- ML: TensorFlow / PyTorch (ì´ìƒ ê°ì§€)
- Speech-to-Text: Google Cloud Speech API
- Image Recognition: AWS Rekognition

**API Endpoints**:
```python
# ì¼€ì–´ ê¸°ë¡
POST   /api/v1/care-reports                    # ì¼€ì–´ ê¸°ë¡ ì‘ì„±
GET    /api/v1/care-reports/{user_id}/daily    # ì¼ë³„ ë¦¬í¬íŠ¸
GET    /api/v1/care-reports/{user_id}/weekly   # ì£¼ê°„ ìš”ì•½
GET    /api/v1/care-reports/{user_id}/monthly  # ì›”ê°„ ìš”ì•½

# AI ë¶„ì„
POST   /api/v1/care-reports/analyze            # AI ë¶„ì„ ìš”ì²­
GET    /api/v1/care-reports/{user_id}/anomalies  # ì´ìƒ íŒ¨í„´

# ë©€í‹°ëª¨ë‹¬ ì…ë ¥
POST   /api/v1/care-reports/voice              # ìŒì„± ê¸°ë¡
POST   /api/v1/care-reports/photo              # ì‚¬ì§„ ê¸°ë¡
POST   /api/v1/care-reports/sensor             # ì„¼ì„œ ë°ì´í„°
```

**Database Schema (TimescaleDB)**:
```sql
-- ì¼€ì–´ ê¸°ë¡
CREATE TABLE care_records (
    record_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL,
    facility_id UUID NOT NULL,
    care_recipient_id UUID,  -- ëŒë´„ ëŒ€ìƒì (ë…¸ì¸, ì•„ë™)
    record_type VARCHAR(50) CHECK (record_type IN ('meal', 'sleep', 'activity', 'medication', 'health', 'behavior', 'mood')),
    recorded_by UUID NOT NULL,  -- ê¸°ë¡ì (ìš”ì–‘ë³´í˜¸ì‚¬, êµì‚¬)
    input_type VARCHAR(20) CHECK (input_type IN ('text', 'voice', 'photo', 'sensor')),
    content TEXT,
    media_urls TEXT[],
    ai_analysis JSONB,
    recorded_at TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- TimescaleDB Hypertable ìƒì„±
SELECT create_hypertable('care_records', 'recorded_at');

-- ì¸ë±ìŠ¤
CREATE INDEX idx_care_records_user ON care_records(user_id, recorded_at DESC);
CREATE INDEX idx_care_records_facility ON care_records(facility_id, recorded_at DESC);
CREATE INDEX idx_care_records_type ON care_records(record_type);

-- ê±´ê°• ë©”íŠ¸ë¦­ (ì‹œê³„ì—´ ë°ì´í„°)
CREATE TABLE health_metrics (
    time TIMESTAMPTZ NOT NULL,
    user_id UUID NOT NULL,
    care_recipient_id UUID NOT NULL,
    metric_type VARCHAR(50) NOT NULL,  -- 'heart_rate', 'blood_pressure', 'temperature', 'weight', 'sleep_hours', 'meal_intake'
    value DECIMAL(10,2) NOT NULL,
    unit VARCHAR(20),
    device_id VARCHAR(100),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Hypertable ìƒì„±
SELECT create_hypertable('health_metrics', 'time');

-- ì¸ë±ìŠ¤
CREATE INDEX idx_health_metrics_recipient ON health_metrics(care_recipient_id, metric_type, time DESC);
```

**AI ì´ìƒ íŒ¨í„´ ê°ì§€**:
```python
import numpy as np
from sklearn.ensemble import IsolationForest
from statsmodels.tsa.seasonal import seasonal_decompose

class AnomalyDetector:
    def __init__(self):
        self.model = IsolationForest(
            contamination=0.05,  # 5% ì´ìƒì¹˜
            random_state=42
        )
        
        self.thresholds = {
            'heart_rate': {'min': 60, 'max': 100},
            'blood_pressure_systolic': {'min': 90, 'max': 140},
            'blood_pressure_diastolic': {'min': 60, 'max': 90},
            'temperature': {'min': 36.0, 'max': 37.5},
            'sleep_hours': {'min': 6, 'max': 10},
            'meal_intake': {'min': 60, 'max': 100}  # % of normal intake
        }
    
    def detect_anomalies(
        self,
        user_id: str,
        metric_type: str,
        days: int = 30
    ) -> Dict:
        """ì´ìƒ íŒ¨í„´ ê°ì§€"""
        
        # 1. ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘
        historical_data = self.get_historical_data(
            user_id,
            metric_type,
            days=days
        )
        
        if len(historical_data) < 10:
            return {'status': 'insufficient_data'}
        
        # 2. í†µê³„ ë¶„ì„
        stats = self.calculate_statistics(historical_data)
        
        # 3. ì ˆëŒ€ ì„ê³„ê°’ ì²´í¬
        threshold_anomalies = self.check_thresholds(
            historical_data[-1],  # ìµœì‹  ê°’
            metric_type
        )
        
        # 4. ìƒëŒ€ ì´ìƒ ê°ì§€ (Z-score)
        z_score_anomalies = self.detect_z_score_anomalies(
            historical_data,
            threshold=2.5
        )
        
        # 5. ì‹œê³„ì—´ ì´ìƒ ê°ì§€ (Isolation Forest)
        ml_anomalies = self.detect_ml_anomalies(historical_data)
        
        # 6. íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = self.analyze_trend(historical_data)
        
        # 7. ì¢…í•© íŒë‹¨
        is_anomaly = (
            threshold_anomalies['is_anomaly'] or
            z_score_anomalies['is_anomaly'] or
            ml_anomalies['is_anomaly']
        )
        
        severity = self.calculate_severity(
            threshold_anomalies,
            z_score_anomalies,
            ml_anomalies
        )
        
        return {
            'is_anomaly': is_anomaly,
            'severity': severity,  # 'low', 'medium', 'high', 'critical'
            'current_value': historical_data[-1],
            'statistics': stats,
            'threshold_check': threshold_anomalies,
            'z_score_check': z_score_anomalies,
            'ml_check': ml_anomalies,
            'trend': trend_analysis,
            'recommendation': self.get_recommendation(
                metric_type,
                is_anomaly,
                severity
            )
        }
    
    def detect_z_score_anomalies(
        self,
        data: List[float],
        threshold: float = 2.5
    ) -> Dict:
        """Z-score ê¸°ë°˜ ì´ìƒ ê°ì§€"""
        
        mean = np.mean(data)
        std = np.std(data)
        
        if std == 0:
            return {'is_anomaly': False, 'z_score': 0}
        
        current_value = data[-1]
        z_score = (current_value - mean) / std
        
        is_anomaly = abs(z_score) > threshold
        
        return {
            'is_anomaly': is_anomaly,
            'z_score': z_score,
            'mean': mean,
            'std': std,
            'threshold': threshold,
            'message': f'{metric_type} ìˆ˜ì¹˜ê°€ í‰ê· ì—ì„œ {abs(z_score):.2f} í‘œì¤€í¸ì°¨ ë²—ì–´ë‚¨'
        }
    
    def detect_ml_anomalies(self, data: List[float]) -> Dict:
        """Isolation Forest ê¸°ë°˜ ì´ìƒ ê°ì§€"""
        
        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        features = self.create_features(data)
        
        # ëª¨ë¸ í•™ìŠµ (ì˜¨ë¼ì¸ í•™ìŠµ)
        self.model.fit(features)
        
        # ìµœì‹  ë°ì´í„° ì˜ˆì¸¡
        prediction = self.model.predict([features[-1]])[0]
        anomaly_score = self.model.score_samples([features[-1]])[0]
        
        is_anomaly = (prediction == -1)
        
        return {
            'is_anomaly': is_anomaly,
            'anomaly_score': anomaly_score,
            'confidence': abs(anomaly_score)
        }
    
    def create_features(self, data: List[float]) -> np.ndarray:
        """ì‹œê³„ì—´ ë°ì´í„° íŠ¹ì„± ìƒì„±"""
        
        features = []
        
        for i in range(1, len(data)):
            feature_vector = [
                data[i],                        # í˜„ì¬ ê°’
                data[i] - data[i-1],           # ë³€í™”ëŸ‰
                np.mean(data[max(0,i-7):i+1]), # 7ì¼ ì´ë™í‰ê· 
                np.std(data[max(0,i-7):i+1])   # 7ì¼ í‘œì¤€í¸ì°¨
            ]
            features.append(feature_vector)
        
        return np.array(features)
    
    def analyze_trend(self, data: List[float]) -> Dict:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        
        if len(data) < 14:
            return {'status': 'insufficient_data'}
        
        # ì‹œê³„ì—´ ë¶„í•´ (íŠ¸ë Œë“œ, ê³„ì ˆì„±, ì”ì°¨)
        decomposition = seasonal_decompose(
            data,
            model='additive',
            period=7  # ì£¼ê°„ íŒ¨í„´
        )
        
        trend = decomposition.trend
        
        # íŠ¸ë Œë“œ ë°©í–¥
        recent_trend = trend[-7:]
        trend_direction = 'increasing' if recent_trend[-1] > recent_trend[0] else 'decreasing'
        trend_strength = abs(recent_trend[-1] - recent_trend[0]) / recent_trend[0]
        
        return {
            'direction': trend_direction,
            'strength': trend_strength,
            'is_significant': trend_strength > 0.1  # 10% ì´ìƒ ë³€í™”
        }
    
    def get_recommendation(
        self,
        metric_type: str,
        is_anomaly: bool,
        severity: str
    ) -> str:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        if not is_anomaly:
            return "ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤."
        
        recommendations = {
            'heart_rate': {
                'high': "ì‹¬ë°•ìˆ˜ê°€ ë†’ìŠµë‹ˆë‹¤. ì˜ë£Œì§„ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                'critical': "ì‹¬ë°•ìˆ˜ê°€ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì˜ë£Œì§„ì—ê²Œ ì—°ë½í•˜ì„¸ìš”."
            },
            'blood_pressure': {
                'high': "í˜ˆì••ì´ ë†’ìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                'critical': "í˜ˆì••ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì˜ë£Œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            },
            'temperature': {
                'high': "ì²´ì˜¨ì´ ë†’ìŠµë‹ˆë‹¤. ë°œì—´ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                'critical': "ê³ ì—´ì…ë‹ˆë‹¤. ì¦‰ì‹œ ì˜ë£Œì§„ì—ê²Œ ì—°ë½í•˜ì„¸ìš”."
            },
            'sleep_hours': {
                'low': "ìˆ˜ë©´ ì‹œê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ìˆ˜ë©´ íŒ¨í„´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
            },
            'meal_intake': {
                'low': "ì‹ì‚¬ëŸ‰ì´ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì‹ìš• ì €í•˜ ì›ì¸ì„ í™•ì¸í•˜ì„¸ìš”."
            }
        }
        
        return recommendations.get(metric_type, {}).get(severity, "ì´ìƒ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

---

ì´ ë¬¸ì„œëŠ” ì—¬ê¸°ê¹Œì§€ì´ë©°, ê³„ì†í•´ì„œ ë‚˜ë¨¸ì§€ ì„œë¹„ìŠ¤ë“¤ (Notification, Payment, Public API Gateway, Analytics, Privacy Service ë“±)ê³¼ ë³´ì•ˆ ì·¨ì•½ì , ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œì„  ë°©ì•ˆì„ ë‹¤ìŒ íŒŒíŠ¸ì—ì„œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

ë¬¸ì„œê°€ ë§¤ìš° ê¸¸ì–´ì„œ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í• ê¹Œìš”?
