# ì„œìš¸ì¼€ì–´í”ŒëŸ¬ìŠ¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œì„  ë°©ì•ˆ

## ğŸ“‹ ëª©ì°¨
1. [í˜„ì¬ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¶„ì„](#1-í˜„ì¬-ë°ì´í„°-íŒŒì´í”„ë¼ì¸-ë¶„ì„)
2. [ê°œì„ ëœ ETL ì•„í‚¤í…ì²˜](#2-ê°œì„ ëœ-etl-ì•„í‚¤í…ì²˜)
3. [ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸](#3-ì‹¤ì‹œê°„-ë°ì´í„°-íŒŒì´í”„ë¼ì¸)
4. [ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬](#4-ë°ì´í„°-í’ˆì§ˆ-ê´€ë¦¬)
5. [ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”](#5-ë°°ì¹˜-ì²˜ë¦¬-ìµœì í™”)
6. [ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼](#6-ëª¨ë‹ˆí„°ë§-ë°-ì•Œë¦¼)

---

## 1. í˜„ì¬ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¶„ì„

### 1.1 ê¸°ì¡´ êµ¬ì¡°ì˜ ë¬¸ì œì 

**âŒ ì‹ë³„ëœ ë¬¸ì œì **:

1. **ë°ì´í„° ë™ê¸°í™” ì „ëµ ë¶€ì¬**
   - ê³µê³µ API ë°ì´í„° ê°±ì‹  ì£¼ê¸° ë¯¸ì •ì˜
   - ì¦ë¶„ ì—…ë°ì´íŠ¸ vs ì „ì²´ ì—…ë°ì´íŠ¸ ì „ëµ ì—†ìŒ
   - ë³€ê²½ ê°ì§€ (CDC) ë©”ì»¤ë‹ˆì¦˜ ì—†ìŒ

2. **ì—ëŸ¬ ì²˜ë¦¬ ë¶€ì¡±**
   - API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ ì—†ìŒ
   - Dead Letter Queue (DLQ) ë¯¸êµ¬í˜„
   - ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬ ì „ëµ ì—†ìŒ

3. **ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë¯¸í¡**
   - ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì—†ìŒ
   - ì´ìƒì¹˜ íƒì§€ ì—†ìŒ
   - ë°ì´í„° ì¼ê´€ì„± ì²´í¬ ì—†ìŒ

4. **ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ë¶€ì¬**
   - íŒŒì´í”„ë¼ì¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì—†ìŒ
   - ì‹¤íŒ¨ ì•Œë¦¼ ì‹œìŠ¤í…œ ì—†ìŒ
   - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì—†ìŒ

5. **í™•ì¥ì„± ì œí•œ**
   - ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰
   - ë³‘ë ¬ ì²˜ë¦¬ ì—†ìŒ
   - ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ë¶ˆê°€

---

## 2. ê°œì„ ëœ ETL ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Sources (ê³µê³µ API)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ë³´ê±´ë³µì§€ë¶€ API  â”‚  â”‚ êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨ â”‚  â”‚ ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥      â”‚ â”‚
â”‚  â”‚ (ì–´ë¦°ì´ì§‘)      â”‚  â”‚ (ìš”ì–‘ì‹œì„¤)       â”‚  â”‚ (ì§€ì—­ì„¼í„°, ë³µì§€ì‹œì„¤)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                 â”‚                  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Gateway & Rate Limiter                               â”‚
â”‚  - Request Throttling                                                       â”‚
â”‚  - API Key Rotation                                                         â”‚
â”‚  - Circuit Breaker Pattern                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Ingestion Layer                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Apache Airflow DAGs                                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Childcare    â”‚  â”‚ Nursing Home â”‚  â”‚  Community Center       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Sync DAG     â”‚  â”‚  Sync DAG    â”‚  â”‚  Sync DAG               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ (Daily 3AM)  â”‚  â”‚ (Weekly Sun) â”‚  â”‚  (Hourly)               â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Data Processing Layer                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Apache Spark / Pandas                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  Transform   â”‚  â”‚  Validate    â”‚  â”‚  Enrich                   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  (Normalize) â”‚  â”‚  (Quality)   â”‚  â”‚  (Geocoding, Scoring)     â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Change Data Capture (CDC)                              â”‚
â”‚  - Detect Changes (Insert/Update/Delete)                                   â”‚
â”‚  - Generate Delta Records                                                   â”‚
â”‚  - Version Management                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Storage Layer                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  Data Lake   â”‚  â”‚  PostgreSQL  â”‚  â”‚  Elasticsearch             â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  (S3/Parquet)â”‚  â”‚  (OLTP)      â”‚  â”‚  (Search)                  â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚  â”‚
â”‚  â”‚  â”‚   Redis      â”‚  â”‚   BigQuery   â”‚                                  â”‚  â”‚
â”‚  â”‚  â”‚   (Cache)    â”‚  â”‚   (OLAP)     â”‚                                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Downstream Consumers                                    â”‚
â”‚  - API Services                                                             â”‚
â”‚  - Analytics Dashboard                                                      â”‚
â”‚  - ML Training Pipeline                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Apache Airflow DAG êµ¬í˜„

**ì–´ë¦°ì´ì§‘ ë°ì´í„° ë™ê¸°í™” DAG**:

```python
# dags/sync_childcare_facilities.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.http.sensors.http import HttpSensor
from airflow.utils.dates import days_ago
from datetime import timedelta
import logging

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email': ['alerts@seoulcareplus.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2)
}

dag = DAG(
    'sync_childcare_facilities',
    default_args=default_args,
    description='ë³´ê±´ë³µì§€ë¶€ ì–´ë¦°ì´ì§‘ ë°ì´í„° ë™ê¸°í™”',
    schedule_interval='0 3 * * *',  # ë§¤ì¼ ìƒˆë²½ 3ì‹œ
    start_date=days_ago(1),
    catchup=False,
    tags=['public-api', 'childcare', 'daily']
)

# Task 1: API ê±´ê°• ì²´í¬
check_api_health = HttpSensor(
    task_id='check_api_health',
    http_conn_id='mohw_api',
    endpoint='/status',
    request_params={},
    response_check=lambda response: response.status_code == 200,
    poke_interval=30,
    timeout=300,
    dag=dag
)

# Task 2: ë°ì´í„° ì¶”ì¶œ
def extract_childcare_data(**context):
    """ë³´ê±´ë³µì§€ë¶€ APIì—ì„œ ì–´ë¦°ì´ì§‘ ë°ì´í„° ì¶”ì¶œ"""
    from data_pipeline.extractors.childcare_extractor import ChildcareExtractor
    
    extractor = ChildcareExtractor()
    
    try:
        # ì„œìš¸ì‹œ 25ê°œ êµ¬ ë°ì´í„° ì¶”ì¶œ
        seoul_districts = [
            'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬',
            'ë™ëŒ€ë¬¸êµ¬', 'ì¤‘ë‘êµ¬', 'ì„±ë¶êµ¬', 'ê°•ë¶êµ¬', 'ë„ë´‰êµ¬',
            'ë…¸ì›êµ¬', 'ì€í‰êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ë§ˆí¬êµ¬', 'ì–‘ì²œêµ¬',
            'ê°•ì„œêµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ë™ì‘êµ¬',
            'ê´€ì•…êµ¬', 'ì„œì´ˆêµ¬', 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬'
        ]
        
        all_facilities = []
        
        for district in seoul_districts:
            logger.info(f"Extracting data for {district}")
            
            facilities = extractor.fetch_by_district(district)
            all_facilities.extend(facilities)
            
            # API Rate Limiting ì¤€ìˆ˜
            time.sleep(1)
        
        logger.info(f"Extracted {len(all_facilities)} facilities")
        
        # XComìœ¼ë¡œ ë‹¤ìŒ Taskì— ì „ë‹¬
        context['task_instance'].xcom_push(
            key='raw_facilities',
            value=all_facilities
        )
        
        return len(all_facilities)
        
    except Exception as e:
        logger.error(f"Extraction failed: {e}")
        raise

extract_data = PythonOperator(
    task_id='extract_childcare_data',
    python_callable=extract_childcare_data,
    provide_context=True,
    dag=dag
)

# Task 3: ë°ì´í„° ë³€í™˜ ë° ê²€ì¦
def transform_and_validate(**context):
    """ë°ì´í„° ë³€í™˜ ë° í’ˆì§ˆ ê²€ì¦"""
    from data_pipeline.transformers.childcare_transformer import ChildcareTransformer
    from data_pipeline.validators.data_validator import DataValidator
    
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    raw_facilities = context['task_instance'].xcom_pull(
        task_ids='extract_childcare_data',
        key='raw_facilities'
    )
    
    transformer = ChildcareTransformer()
    validator = DataValidator()
    
    # ë°ì´í„° ë³€í™˜
    transformed_facilities = []
    validation_errors = []
    
    for raw_facility in raw_facilities:
        try:
            # ë³€í™˜
            facility = transformer.transform(raw_facility)
            
            # ê²€ì¦
            is_valid, errors = validator.validate_facility(facility)
            
            if is_valid:
                transformed_facilities.append(facility)
            else:
                validation_errors.append({
                    'facility_name': raw_facility.get('stcode_nm'),
                    'errors': errors
                })
                
        except Exception as e:
            logger.error(f"Transform failed for facility: {e}")
            validation_errors.append({
                'facility_name': raw_facility.get('stcode_nm'),
                'error': str(e)
            })
    
    # ê²€ì¦ ì—ëŸ¬ ë¡œê¹…
    if validation_errors:
        logger.warning(f"Validation errors: {len(validation_errors)}")
        for error in validation_errors[:10]:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
            logger.warning(error)
    
    # ì„±ê³µë¥  ì²´í¬
    success_rate = len(transformed_facilities) / len(raw_facilities)
    
    if success_rate < 0.9:  # 90% ë¯¸ë§Œì´ë©´ ì‹¤íŒ¨
        raise ValueError(f"Success rate too low: {success_rate:.2%}")
    
    logger.info(f"Transformed {len(transformed_facilities)} facilities (success rate: {success_rate:.2%})")
    
    # ë‹¤ìŒ Taskë¡œ ì „ë‹¬
    context['task_instance'].xcom_push(
        key='transformed_facilities',
        value=transformed_facilities
    )
    
    return len(transformed_facilities)

transform_data = PythonOperator(
    task_id='transform_and_validate',
    python_callable=transform_and_validate,
    provide_context=True,
    dag=dag
)

# Task 4: CDC (Change Data Capture)
def detect_changes(**context):
    """ë³€ê²½ ì‚¬í•­ ê°ì§€"""
    from data_pipeline.cdc.change_detector import ChangeDetector
    
    transformed_facilities = context['task_instance'].xcom_pull(
        task_ids='transform_and_validate',
        key='transformed_facilities'
    )
    
    detector = ChangeDetector()
    
    # ë³€ê²½ ì‚¬í•­ ê°ì§€
    changes = detector.detect_changes(
        new_data=transformed_facilities,
        data_source='childcare_facilities'
    )
    
    logger.info(f"Changes detected:")
    logger.info(f"  - Inserts: {len(changes['inserts'])}")
    logger.info(f"  - Updates: {len(changes['updates'])}")
    logger.info(f"  - Deletes: {len(changes['deletes'])}")
    
    # ë‹¤ìŒ Taskë¡œ ì „ë‹¬
    context['task_instance'].xcom_push(
        key='changes',
        value=changes
    )
    
    return changes

detect_changes_task = PythonOperator(
    task_id='detect_changes',
    python_callable=detect_changes,
    provide_context=True,
    dag=dag
)

# Task 5: ë°ì´í„°ë² ì´ìŠ¤ ì ì¬
def load_to_database(**context):
    """ë³€ê²½ ì‚¬í•­ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì ì¬"""
    from data_pipeline.loaders.database_loader import DatabaseLoader
    
    changes = context['task_instance'].xcom_pull(
        task_ids='detect_changes',
        key='changes'
    )
    
    loader = DatabaseLoader()
    
    # íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì ì¬
    try:
        loader.begin_transaction()
        
        # Insert
        if changes['inserts']:
            loader.bulk_insert('facilities', changes['inserts'])
            logger.info(f"Inserted {len(changes['inserts'])} facilities")
        
        # Update
        if changes['updates']:
            loader.bulk_update('facilities', changes['updates'])
            logger.info(f"Updated {len(changes['updates'])} facilities")
        
        # Delete (Soft Delete)
        if changes['deletes']:
            loader.soft_delete('facilities', changes['deletes'])
            logger.info(f"Deleted {len(changes['deletes'])} facilities")
        
        loader.commit_transaction()
        
        return {
            'inserts': len(changes['inserts']),
            'updates': len(changes['updates']),
            'deletes': len(changes['deletes'])
        }
        
    except Exception as e:
        loader.rollback_transaction()
        logger.error(f"Load failed: {e}")
        raise

load_data = PythonOperator(
    task_id='load_to_database',
    python_callable=load_to_database,
    provide_context=True,
    dag=dag
)

# Task 6: Elasticsearch ì¸ë±ì‹±
def index_to_elasticsearch(**context):
    """Elasticsearchì— ì¸ë±ì‹±"""
    from data_pipeline.indexers.elasticsearch_indexer import ElasticsearchIndexer
    
    changes = context['task_instance'].xcom_pull(
        task_ids='detect_changes',
        key='changes'
    )
    
    indexer = ElasticsearchIndexer()
    
    # Insert + Updateë§Œ ì¸ë±ì‹±
    documents_to_index = changes['inserts'] + changes['updates']
    
    if documents_to_index:
        indexer.bulk_index(
            index='facilities',
            documents=documents_to_index
        )
        logger.info(f"Indexed {len(documents_to_index)} documents to Elasticsearch")
    
    # DeleteëŠ” ì œê±°
    if changes['deletes']:
        indexer.bulk_delete(
            index='facilities',
            document_ids=[d['facility_id'] for d in changes['deletes']]
        )
        logger.info(f"Deleted {len(changes['deletes'])} documents from Elasticsearch")
    
    return len(documents_to_index)

index_data = PythonOperator(
    task_id='index_to_elasticsearch',
    python_callable=index_to_elasticsearch,
    provide_context=True,
    dag=dag
)

# Task 7: ìºì‹œ ë¬´íš¨í™”
def invalidate_cache(**context):
    """Redis ìºì‹œ ë¬´íš¨í™”"""
    import redis
    
    redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    # ì‹œì„¤ ê´€ë ¨ ìºì‹œ í‚¤ íŒ¨í„´
    cache_patterns = [
        'facilities:*',
        'search:*',
        'nearby:*'
    ]
    
    invalidated_count = 0
    
    for pattern in cache_patterns:
        keys = redis_client.keys(pattern)
        if keys:
            redis_client.delete(*keys)
            invalidated_count += len(keys)
    
    logger.info(f"Invalidated {invalidated_count} cache keys")
    
    return invalidated_count

invalidate_cache_task = PythonOperator(
    task_id='invalidate_cache',
    python_callable=invalidate_cache,
    provide_context=True,
    dag=dag
)

# Task 8: ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê¸°ë¡
def record_metrics(**context):
    """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê¸°ë¡"""
    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
    
    load_result = context['task_instance'].xcom_pull(
        task_ids='load_to_database'
    )
    
    registry = CollectorRegistry()
    
    # Prometheus ë©”íŠ¸ë¦­
    inserts_gauge = Gauge(
        'data_pipeline_inserts_total',
        'Total inserts',
        ['source'],
        registry=registry
    )
    updates_gauge = Gauge(
        'data_pipeline_updates_total',
        'Total updates',
        ['source'],
        registry=registry
    )
    deletes_gauge = Gauge(
        'data_pipeline_deletes_total',
        'Total deletes',
        ['source'],
        registry=registry
    )
    
    inserts_gauge.labels(source='childcare').set(load_result['inserts'])
    updates_gauge.labels(source='childcare').set(load_result['updates'])
    deletes_gauge.labels(source='childcare').set(load_result['deletes'])
    
    # Pushgatewayë¡œ ì „ì†¡
    push_to_gateway(
        'localhost:9091',
        job='data_pipeline',
        registry=registry
    )
    
    logger.info("Metrics pushed to Prometheus")

record_metrics_task = PythonOperator(
    task_id='record_metrics',
    python_callable=record_metrics,
    provide_context=True,
    dag=dag
)

# Task 9: ì•Œë¦¼ ë°œì†¡
def send_notification(**context):
    """ì²˜ë¦¬ ì™„ë£Œ ì•Œë¦¼"""
    from data_pipeline.notifiers.slack_notifier import SlackNotifier
    
    load_result = context['task_instance'].xcom_pull(
        task_ids='load_to_database'
    )
    
    notifier = SlackNotifier()
    
    message = f"""
    âœ… ì–´ë¦°ì´ì§‘ ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ
    
    ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:
    - ì‹ ê·œ: {load_result['inserts']}ê°œ
    - ìˆ˜ì •: {load_result['updates']}ê°œ
    - ì‚­ì œ: {load_result['deletes']}ê°œ
    
    â° ì™„ë£Œ ì‹œê°„: {context['execution_date'].strftime('%Y-%m-%d %H:%M:%S')}
    """
    
    notifier.send(
        channel='#data-pipeline',
        message=message
    )

send_notification_task = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    provide_context=True,
    dag=dag
)

# DAG ì˜ì¡´ì„± ì •ì˜
check_api_health >> extract_data >> transform_data >> detect_changes_task
detect_changes_task >> [load_data, index_data]
load_data >> invalidate_cache_task >> record_metrics_task >> send_notification_task
index_data >> record_metrics_task
```

### 2.3 CDC (Change Data Capture) êµ¬í˜„

```python
# data_pipeline/cdc/change_detector.py
from typing import Dict, List
import hashlib
import json

class ChangeDetector:
    """ë°ì´í„° ë³€ê²½ ê°ì§€"""
    
    def __init__(self, db_connection):
        self.db = db_connection
    
    def detect_changes(
        self,
        new_data: List[Dict],
        data_source: str
    ) -> Dict[str, List[Dict]]:
        """
        ì‹ ê·œ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ë³€ê²½ ì‚¬í•­ ê°ì§€
        
        Returns:
            {
                'inserts': [],  # ì‹ ê·œ
                'updates': [],  # ìˆ˜ì •
                'deletes': []   # ì‚­ì œ
            }
        """
        
        # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_data = self._load_existing_data(data_source)
        
        # 2. í•´ì‹œë§µ ìƒì„± (ë¹ ë¥¸ ì¡°íšŒ)
        existing_map = {
            record['public_api_id']: record 
            for record in existing_data
        }
        
        new_map = {
            record['public_api_id']: record 
            for record in new_data
        }
        
        # 3. ë³€ê²½ ì‚¬í•­ ë¶„ë¥˜
        inserts = []
        updates = []
        deletes = []
        
        # ì‹ ê·œ ë° ìˆ˜ì •
        for api_id, new_record in new_map.items():
            if api_id not in existing_map:
                # ì‹ ê·œ
                inserts.append(new_record)
            else:
                # ìˆ˜ì • ì—¬ë¶€ í™•ì¸ (í•´ì‹œ ë¹„êµ)
                existing_record = existing_map[api_id]
                
                if self._has_changed(existing_record, new_record):
                    updates.append({
                        **new_record,
                        'facility_id': existing_record['facility_id']  # ê¸°ì¡´ ID ìœ ì§€
                    })
        
        # ì‚­ì œ (ê¸°ì¡´ì—ëŠ” ìˆì§€ë§Œ ì‹ ê·œ ë°ì´í„°ì— ì—†ìŒ)
        for api_id, existing_record in existing_map.items():
            if api_id not in new_map:
                deletes.append(existing_record)
        
        return {
            'inserts': inserts,
            'updates': updates,
            'deletes': deletes
        }
    
    def _load_existing_data(self, data_source: str) -> List[Dict]:
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        query = """
        SELECT facility_id, public_api_id, name, address, facility_data, data_hash
        FROM facilities
        WHERE public_api_source = :source
        AND deleted_at IS NULL
        """
        
        return self.db.fetch_all(query, {"source": data_source})
    
    def _has_changed(self, existing: Dict, new: Dict) -> bool:
        """ë°ì´í„° ë³€ê²½ ì—¬ë¶€ í™•ì¸ (í•´ì‹œ ë¹„êµ)"""
        existing_hash = existing.get('data_hash')
        new_hash = self._calculate_hash(new)
        
        return existing_hash != new_hash
    
    def _calculate_hash(self, data: Dict) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)"""
        # ì¼ë¶€ í•„ë“œë§Œ í•´ì‹œ ê³„ì‚° (ë©”íƒ€ë°ì´í„° ì œì™¸)
        relevant_fields = {
            'name': data.get('name'),
            'address': data.get('address'),
            'capacity': data.get('capacity'),
            'grade': data.get('grade'),
            'facility_data': data.get('facility_data')
        }
        
        # JSON ì •ê·œí™” í›„ í•´ì‹œ
        normalized = json.dumps(relevant_fields, sort_keys=True, ensure_ascii=False)
        
        return hashlib.sha256(normalized.encode()).hexdigest()
```

---

## 3. ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸

### 3.1 Apache Kafka ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°

```python
# data_pipeline/streaming/kafka_producer.py
from aiokafka import AIOKafkaProducer
import json
import logging

logger = logging.getLogger(__name__)

class FacilityEventProducer:
    """ì‹œì„¤ ì´ë²¤íŠ¸ Kafka Producer"""
    
    def __init__(self, bootstrap_servers: str):
        self.bootstrap_servers = bootstrap_servers
        self.producer = None
    
    async def start(self):
        """Producer ì‹œì‘"""
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            compression_type='gzip',
            acks='all',  # ëª¨ë“  replica í™•ì¸
            retries=3
        )
        
        await self.producer.start()
        logger.info("Kafka producer started")
    
    async def stop(self):
        """Producer ì¢…ë£Œ"""
        if self.producer:
            await self.producer.stop()
            logger.info("Kafka producer stopped")
    
    async def send_facility_created(self, facility: Dict):
        """ì‹œì„¤ ìƒì„± ì´ë²¤íŠ¸"""
        event = {
            'event_type': 'facility.created',
            'facility_id': facility['facility_id'],
            'facility_type': facility['facility_type'],
            'data': facility,
            'timestamp': datetime.now().isoformat()
        }
        
        await self.producer.send(
            topic='facility-events',
            value=event,
            key=facility['facility_id'].encode('utf-8')
        )
        
        logger.info(f"Sent facility.created event: {facility['facility_id']}")
    
    async def send_facility_updated(self, facility: Dict, changes: Dict):
        """ì‹œì„¤ ìˆ˜ì • ì´ë²¤íŠ¸"""
        event = {
            'event_type': 'facility.updated',
            'facility_id': facility['facility_id'],
            'changes': changes,
            'timestamp': datetime.now().isoformat()
        }
        
        await self.producer.send(
            topic='facility-events',
            value=event,
            key=facility['facility_id'].encode('utf-8')
        )
    
    async def send_review_created(self, review: Dict):
        """ë¦¬ë·° ì‘ì„± ì´ë²¤íŠ¸"""
        event = {
            'event_type': 'review.created',
            'review_id': review['review_id'],
            'facility_id': review['facility_id'],
            'rating': review['rating'],
            'timestamp': datetime.now().isoformat()
        }
        
        await self.producer.send(
            topic='review-events',
            value=event,
            key=review['facility_id'].encode('utf-8')
        )

# data_pipeline/streaming/kafka_consumer.py
from aiokafka import AIOKafkaConsumer
import asyncio

class FacilityEventConsumer:
    """ì‹œì„¤ ì´ë²¤íŠ¸ Kafka Consumer"""
    
    def __init__(self, bootstrap_servers: str, group_id: str):
        self.bootstrap_servers = bootstrap_servers
        self.group_id = group_id
        self.consumer = None
    
    async def start(self):
        """Consumer ì‹œì‘"""
        self.consumer = AIOKafkaConsumer(
            'facility-events',
            'review-events',
            bootstrap_servers=self.bootstrap_servers,
            group_id=self.group_id,
            value_deserializer=lambda v: json.loads(v.decode('utf-8')),
            enable_auto_commit=False,  # ìˆ˜ë™ ì»¤ë°‹
            auto_offset_reset='earliest'
        )
        
        await self.consumer.start()
        logger.info("Kafka consumer started")
    
    async def stop(self):
        """Consumer ì¢…ë£Œ"""
        if self.consumer:
            await self.consumer.stop()
            logger.info("Kafka consumer stopped")
    
    async def consume(self):
        """ì´ë²¤íŠ¸ ì†Œë¹„"""
        try:
            async for message in self.consumer:
                event = message.value
                
                try:
                    # ì´ë²¤íŠ¸ ì²˜ë¦¬
                    await self.process_event(event)
                    
                    # ì²˜ë¦¬ ì™„ë£Œ í›„ ì»¤ë°‹
                    await self.consumer.commit()
                    
                except Exception as e:
                    logger.error(f"Event processing failed: {e}")
                    
                    # DLQë¡œ ì´ë™
                    await self.send_to_dlq(message)
                    
        except asyncio.CancelledError:
            logger.info("Consumer cancelled")
        except Exception as e:
            logger.error(f"Consumer error: {e}")
            raise
    
    async def process_event(self, event: Dict):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        event_type = event['event_type']
        
        handlers = {
            'facility.created': self.handle_facility_created,
            'facility.updated': self.handle_facility_updated,
            'review.created': self.handle_review_created
        }
        
        handler = handlers.get(event_type)
        
        if handler:
            await handler(event)
        else:
            logger.warning(f"Unknown event type: {event_type}")
    
    async def handle_facility_created(self, event: Dict):
        """ì‹œì„¤ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        facility = event['data']
        
        # 1. Elasticsearch ì¸ë±ì‹±
        await index_to_elasticsearch(facility)
        
        # 2. ìºì‹œ ì›Œë°
        await warm_cache(facility)
        
        # 3. ì•Œë¦¼ ë°œì†¡ (ì£¼ë³€ ì‚¬ìš©ì)
        await notify_nearby_users(facility)
    
    async def handle_review_created(self, event: Dict):
        """ë¦¬ë·° ì‘ì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        review = event
        
        # 1. ì‹œì„¤ í‰ì  ì¬ê³„ì‚°
        await recalculate_facility_rating(review['facility_id'])
        
        # 2. ìºì‹œ ë¬´íš¨í™”
        await invalidate_facility_cache(review['facility_id'])
        
        # 3. ì‹œì„¤ ìš´ì˜ì ì•Œë¦¼
        await notify_facility_owner(review['facility_id'], review['review_id'])
```

---

## 4. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

### 4.1 ë°ì´í„° ê²€ì¦ í”„ë ˆì„ì›Œí¬

```python
# data_pipeline/validators/data_validator.py
from typing import Dict, List, Tuple
import re
from datetime import datetime

class DataValidator:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    
    def validate_facility(self, facility: Dict) -> Tuple[bool, List[str]]:
        """ì‹œì„¤ ë°ì´í„° ê²€ì¦"""
        errors = []
        
        # 1. í•„ìˆ˜ í•„ë“œ ì²´í¬
        required_fields = ['name', 'address', 'location', 'facility_type']
        
        for field in required_fields:
            if not facility.get(field):
                errors.append(f"Missing required field: {field}")
        
        # 2. ë°ì´í„° íƒ€ì… ê²€ì¦
        if facility.get('capacity') and not isinstance(facility['capacity'], int):
            errors.append("capacity must be integer")
        
        if facility.get('rating') and not (0 <= facility['rating'] <= 5):
            errors.append("rating must be between 0 and 5")
        
        # 3. í¬ë§· ê²€ì¦
        if facility.get('phone'):
            if not self._validate_phone(facility['phone']):
                errors.append("Invalid phone format")
        
        if facility.get('email'):
            if not self._validate_email(facility['email']):
                errors.append("Invalid email format")
        
        # 4. ìœ„ì¹˜ ì •ë³´ ê²€ì¦
        if facility.get('location'):
            if not self._validate_location(facility['location']):
                errors.append("Invalid location coordinates")
        
        # 5. ë¹„ì¦ˆë‹ˆìŠ¤ ë£° ê²€ì¦
        if facility.get('facility_type') == 'childcare':
            if not facility.get('capacity'):
                errors.append("Childcare facility must have capacity")
        
        # 6. ë°ì´í„° ì¼ê´€ì„± ì²´í¬
        if facility.get('current_occupancy') and facility.get('capacity'):
            if facility['current_occupancy'] > facility['capacity']:
                errors.append("Current occupancy exceeds capacity")
        
        is_valid = len(errors) == 0
        
        return is_valid, errors
    
    def _validate_phone(self, phone: str) -> bool:
        """ì „í™”ë²ˆí˜¸ ê²€ì¦"""
        pattern = r'^\d{2,3}-\d{3,4}-\d{4}$'
        return bool(re.match(pattern, phone))
    
    def _validate_email(self, email: str) -> bool:
        """ì´ë©”ì¼ ê²€ì¦"""
        pattern = r'^[\w\.-]+@[\w\.-]+\.\w+$'
        return bool(re.match(pattern, email))
    
    def _validate_location(self, location: Dict) -> bool:
        """ìœ„ì¹˜ ì •ë³´ ê²€ì¦ (ì„œìš¸ì‹œ ë²”ìœ„)"""
        lat = location.get('lat')
        lng = location.get('lng')
        
        if not (isinstance(lat, (int, float)) and isinstance(lng, (int, float))):
            return False
        
        # ì„œìš¸ì‹œ ëŒ€ëµì  ë²”ìœ„
        seoul_bounds = {
            'lat_min': 37.4,
            'lat_max': 37.7,
            'lng_min': 126.7,
            'lng_max': 127.2
        }
        
        return (
            seoul_bounds['lat_min'] <= lat <= seoul_bounds['lat_max'] and
            seoul_bounds['lng_min'] <= lng <= seoul_bounds['lng_max']
        )
    
    def calculate_quality_score(self, facility: Dict) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 0
        
        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ (40ì )
        required_fields = ['name', 'address', 'location', 'facility_type']
        filled_required = sum(1 for f in required_fields if facility.get(f))
        score += (filled_required / len(required_fields)) * 40
        
        # ì„ íƒ í•„ë“œ ì¡´ì¬ (30ì )
        optional_fields = ['phone', 'email', 'website', 'description', 'photos']
        filled_optional = sum(1 for f in optional_fields if facility.get(f))
        score += (filled_optional / len(optional_fields)) * 30
        
        # ë°ì´í„° ìµœì‹ ì„± (20ì )
        if facility.get('last_synced_at'):
            days_old = (datetime.now() - facility['last_synced_at']).days
            freshness_score = max(20 - days_old, 0)
            score += freshness_score
        
        # ë°ì´í„° ì •í™•ì„± (10ì )
        # - ë¦¬ë·° ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì •í™•ì„± ë†’ìŒ
        review_count = facility.get('review_count', 0)
        accuracy_score = min(review_count / 10, 10)
        score += accuracy_score
        
        return min(score, 100)
```

### 4.2 ë°ì´í„° í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ

```python
# data_pipeline/monitoring/quality_dashboard.py
class QualityDashboard:
    """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
    
    async def get_quality_metrics(self) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        
        # 1. ì „ì²´ ì‹œì„¤ ìˆ˜
        total_facilities = await db.fetch_val(
            "SELECT COUNT(*) FROM facilities WHERE deleted_at IS NULL"
        )
        
        # 2. ì™„ì „ì„± (Completeness)
        completeness = await self._calculate_completeness()
        
        # 3. ì •í™•ì„± (Accuracy)
        accuracy = await self._calculate_accuracy()
        
        # 4. ìµœì‹ ì„± (Freshness)
        freshness = await self._calculate_freshness()
        
        # 5. ì¼ê´€ì„± (Consistency)
        consistency = await self._calculate_consistency()
        
        # 6. ì¤‘ë³µ (Duplicates)
        duplicates = await self._detect_duplicates()
        
        return {
            'total_facilities': total_facilities,
            'completeness': completeness,
            'accuracy': accuracy,
            'freshness': freshness,
            'consistency': consistency,
            'duplicates': duplicates,
            'overall_score': self._calculate_overall_score(
                completeness,
                accuracy,
                freshness,
                consistency
            )
        }
    
    async def _calculate_completeness(self) -> Dict:
        """ì™„ì „ì„± ê³„ì‚°"""
        query = """
        SELECT 
            COUNT(*) as total,
            COUNT(phone) as with_phone,
            COUNT(email) as with_email,
            COUNT(website) as with_website,
            COUNT(CASE WHEN JSONB_ARRAY_LENGTH(photos) > 0 THEN 1 END) as with_photos
        FROM facilities
        WHERE deleted_at IS NULL
        """
        
        result = await db.fetch_one(query)
        
        return {
            'phone_rate': result['with_phone'] / result['total'],
            'email_rate': result['with_email'] / result['total'],
            'website_rate': result['with_website'] / result['total'],
            'photos_rate': result['with_photos'] / result['total']
        }
    
    async def _calculate_freshness(self) -> Dict:
        """ìµœì‹ ì„± ê³„ì‚°"""
        query = """
        SELECT 
            COUNT(*) FILTER (WHERE last_synced_at >= NOW() - INTERVAL '1 day') as last_24h,
            COUNT(*) FILTER (WHERE last_synced_at >= NOW() - INTERVAL '7 days') as last_7d,
            COUNT(*) FILTER (WHERE last_synced_at >= NOW() - INTERVAL '30 days') as last_30d,
            COUNT(*) as total
        FROM facilities
        WHERE deleted_at IS NULL
        """
        
        result = await db.fetch_one(query)
        
        return {
            'last_24h_rate': result['last_24h'] / result['total'],
            'last_7d_rate': result['last_7d'] / result['total'],
            'last_30d_rate': result['last_30d'] / result['total']
        }
```

---

ì´ ë¬¸ì„œëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì£¼ìš” ê°œì„  ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ **ì‹œê°ì  ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨**ê³¼ **ì„¸ë¶„í™”ê°€ í•„ìš”í•œ ê¸°ëŠ¥**ì„ ë³„ë„ ë¬¸ì„œë¡œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.
